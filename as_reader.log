03-01-2018:00:34:23,725 INFO     [main.py:173] in main
03-01-2018:00:34:23,725 INFO     [main.py:87] Loading existing vocab and w2i
03-01-2018:00:34:23,862 INFO     [data_utils.py:166] Vocabulary size = 54235
03-01-2018:00:34:24,223 INFO     [data_utils.py:179] w2i size = 54235
03-01-2018:00:36:37,730 INFO     [data_utils.py:206] Number of data points loaded = 107944
03-01-2018:00:36:39,929 INFO     [data_utils.py:206] Number of data points loaded = 1991
03-01-2018:00:36:42,752 INFO     [data_utils.py:206] Number of data points loaded = 2491
03-01-2018:00:36:42,752 INFO     [main.py:111] Vocab size = 54235
03-01-2018:00:36:42,752 INFO     [main.py:112] Number of training data points = 107944
03-01-2018:00:36:42,752 INFO     [main.py:113] Number of validation data points = 1991
03-01-2018:00:36:42,752 INFO     [main.py:114] Number of testing data points = 2491
03-01-2018:00:36:42,752 INFO     [ASReaderModel.py:66] Creating the model...
03-01-2018:00:36:42,800 INFO     [ASReaderModel.py:99] Done creating the model
03-01-2018:00:36:42,800 INFO     [ASReaderTrainer.py:101] Starting to train
03-01-2018:00:36:43,203 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 0, total_loss/examples = 5.40524864197
03-01-2018:00:36:43,203 INFO     [ASReaderTrainer.py:191] Starting to calculate accuracy
03-01-2018:00:36:43,203 INFO     [ASReaderTrainer.py:185] Starting to predict
03-01-2018:00:39:10,364 INFO     [ASReaderTrainer.py:146] valid_accuracy = 0.225514816675
03-01-2018:00:39:10,364 INFO     [ASReaderTrainer.py:149] previous valid accuracy = 0.0, current valid accuracy = 0.225514816675
03-01-2018:00:39:10,364 INFO     [ASReaderTrainer.py:215] Saving model in file: /home/shantanu/PycharmProjects/attentionSum/generated_data/model_save.txt
03-01-2018:00:39:13,175 INFO     [ASReaderTrainer.py:218] Saving model args in file: /home/shantanu/PycharmProjects/attentionSum/generated_data/model_args_save.txt
03-01-2018:00:39:13,175 INFO     [ASReaderTrainer.py:221] Done saving model and model args
03-01-2018:00:39:19,395 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 10, total_loss/examples = 5.46300799196
03-01-2018:00:39:26,234 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 20, total_loss/examples = 4.9728113129
03-01-2018:00:39:33,681 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 30, total_loss/examples = 4.75603532791
03-01-2018:00:39:41,341 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 40, total_loss/examples = 4.65364723089
03-01-2018:00:39:46,578 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 50, total_loss/examples = 4.58587808235
03-01-2018:00:39:52,795 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 60, total_loss/examples = 4.49682546444
03-01-2018:00:39:59,204 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 70, total_loss/examples = 4.43812067744
03-01-2018:00:40:05,253 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 80, total_loss/examples = 4.36021354758
03-01-2018:00:40:11,288 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 90, total_loss/examples = 4.30762731112
03-01-2018:00:40:17,597 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 100, total_loss/examples = 4.25375424989
03-01-2018:00:40:23,721 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 110, total_loss/examples = 4.21157461244
03-01-2018:00:40:29,849 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 120, total_loss/examples = 4.16870864955
03-01-2018:00:40:35,908 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 130, total_loss/examples = 4.10067095101
03-01-2018:00:40:42,120 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 140, total_loss/examples = 4.0352062063
03-01-2018:00:40:48,448 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 150, total_loss/examples = 4.00149590922
03-01-2018:00:40:53,729 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 160, total_loss/examples = 3.99077845064
03-01-2018:00:40:59,789 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 170, total_loss/examples = 3.95919821137
03-01-2018:00:41:05,811 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 180, total_loss/examples = 3.91935623417
03-01-2018:00:41:11,788 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 190, total_loss/examples = 3.88954333984
03-01-2018:00:41:17,828 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 200, total_loss/examples = 3.86018808801
03-01-2018:00:41:23,814 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 210, total_loss/examples = 3.83859451574
03-01-2018:00:41:29,720 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 220, total_loss/examples = 3.80509827903
03-01-2018:00:41:35,616 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 230, total_loss/examples = 3.77907982843
03-01-2018:00:41:42,123 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 240, total_loss/examples = 3.75569473065
03-01-2018:00:41:48,112 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 250, total_loss/examples = 3.73235980163
03-01-2018:00:41:53,308 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 260, total_loss/examples = 3.70843890344
03-01-2018:00:41:59,541 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 270, total_loss/examples = 3.69073305271
03-01-2018:00:42:06,80 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 280, total_loss/examples = 3.67411816756
03-01-2018:00:42:12,126 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 290, total_loss/examples = 3.65288581307
03-01-2018:00:42:18,61 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 300, total_loss/examples = 3.63190773516
03-01-2018:00:42:24,238 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 310, total_loss/examples = 3.61437492662
03-01-2018:00:42:30,185 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 320, total_loss/examples = 3.59301504205
03-01-2018:00:42:36,501 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 330, total_loss/examples = 3.58344705912
03-01-2018:00:42:42,716 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 340, total_loss/examples = 3.5716899102
03-01-2018:00:42:48,814 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 350, total_loss/examples = 3.55219868208
03-01-2018:00:42:53,946 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 360, total_loss/examples = 3.52992490751
03-01-2018:00:42:59,872 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 370, total_loss/examples = 3.51593527929
03-01-2018:00:43:06,124 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 380, total_loss/examples = 3.5074084021
03-01-2018:00:43:12,62 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 390, total_loss/examples = 3.49421023377
03-01-2018:00:43:18,778 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 400, total_loss/examples = 3.4843443971
03-01-2018:00:43:24,915 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 410, total_loss/examples = 3.47452687956
03-01-2018:00:43:31,34 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 420, total_loss/examples = 3.45955083348
03-01-2018:00:43:37,480 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 430, total_loss/examples = 3.45539250368
03-01-2018:00:43:43,633 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 440, total_loss/examples = 3.45138005145
03-01-2018:00:43:49,812 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 450, total_loss/examples = 3.44184143422
03-01-2018:00:43:56,39 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 460, total_loss/examples = 3.43507123097
03-01-2018:00:44:00,971 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 470, total_loss/examples = 3.42939623501
03-01-2018:00:44:06,924 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 480, total_loss/examples = 3.41672742491
03-01-2018:00:44:12,902 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 490, total_loss/examples = 3.40033827343
03-01-2018:00:44:18,928 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 500, total_loss/examples = 3.38622767388
03-01-2018:00:44:25,14 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 510, total_loss/examples = 3.3752459346
03-01-2018:00:44:31,216 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 520, total_loss/examples = 3.36276088383
03-01-2018:00:44:37,332 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 530, total_loss/examples = 3.35331676774
03-01-2018:00:44:43,569 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 540, total_loss/examples = 3.34373625854
03-01-2018:00:44:49,759 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 550, total_loss/examples = 3.33559602748
03-01-2018:00:44:55,943 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 560, total_loss/examples = 3.32941573019
03-01-2018:00:45:02,288 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 570, total_loss/examples = 3.32210877981
03-01-2018:00:45:08,338 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 580, total_loss/examples = 3.31220796227
03-01-2018:00:45:14,655 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 590, total_loss/examples = 3.30080210618
03-01-2018:00:45:20,804 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 600, total_loss/examples = 3.29433739542
03-01-2018:00:45:26,782 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 610, total_loss/examples = 3.28798198719
03-01-2018:00:45:31,757 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 620, total_loss/examples = 3.27933322692
03-01-2018:00:45:37,829 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 630, total_loss/examples = 3.27041845786
03-01-2018:00:45:43,782 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 640, total_loss/examples = 3.26387957292
03-01-2018:00:45:49,996 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 650, total_loss/examples = 3.25832479414
03-01-2018:00:45:56,504 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 660, total_loss/examples = 3.24776241567
03-01-2018:00:46:02,610 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 670, total_loss/examples = 3.23763421928
03-01-2018:00:46:08,703 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 680, total_loss/examples = 3.23242605993
03-01-2018:00:46:14,886 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 690, total_loss/examples = 3.22268041602
03-01-2018:00:46:20,968 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 700, total_loss/examples = 3.21893436609
03-01-2018:00:46:27,529 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 710, total_loss/examples = 3.21257684402
03-01-2018:00:46:34,95 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 720, total_loss/examples = 3.206582949
03-01-2018:00:46:40,149 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 730, total_loss/examples = 3.19825084346
03-01-2018:00:46:45,196 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 740, total_loss/examples = 3.19408595723
03-01-2018:00:46:51,315 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 750, total_loss/examples = 3.18720532797
03-01-2018:00:46:57,208 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 760, total_loss/examples = 3.17758861086
03-01-2018:00:47:03,502 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 770, total_loss/examples = 3.17100320731
03-01-2018:00:47:09,573 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 780, total_loss/examples = 3.1622782474
03-01-2018:00:47:15,810 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 790, total_loss/examples = 3.15850282198
03-01-2018:00:47:21,907 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 800, total_loss/examples = 3.14954698145
03-01-2018:00:47:28,4 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 810, total_loss/examples = 3.14335634485
03-01-2018:00:47:34,740 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 820, total_loss/examples = 3.13987587096
03-01-2018:00:47:40,986 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 830, total_loss/examples = 3.13519112829
03-01-2018:00:47:47,62 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 840, total_loss/examples = 3.12790639945
03-01-2018:00:47:52,196 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 850, total_loss/examples = 3.11924271816
03-01-2018:00:47:58,375 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 860, total_loss/examples = 3.11139649755
03-01-2018:00:48:04,379 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 870, total_loss/examples = 3.10510549849
03-01-2018:00:48:10,512 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 880, total_loss/examples = 3.09734325796
03-01-2018:00:48:16,592 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 890, total_loss/examples = 3.09306441921
03-01-2018:00:48:22,862 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 900, total_loss/examples = 3.08956469071
03-01-2018:00:48:28,760 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 910, total_loss/examples = 3.08196986049
03-01-2018:00:48:34,760 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 920, total_loss/examples = 3.07643177636
03-01-2018:00:48:40,875 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 930, total_loss/examples = 3.07076599198
03-01-2018:00:48:45,990 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 940, total_loss/examples = 3.06902698287
03-01-2018:00:48:53,382 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 950, total_loss/examples = 3.06439345918
03-01-2018:00:48:58,516 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 960, total_loss/examples = 3.0601865367
03-01-2018:00:49:05,410 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 970, total_loss/examples = 3.05766343714
03-01-2018:00:49:10,638 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 980, total_loss/examples = 3.05157847324
03-01-2018:00:49:16,744 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 990, total_loss/examples = 3.04628888136
03-01-2018:00:49:22,648 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1000, total_loss/examples = 3.03848619323
03-01-2018:00:49:28,481 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1010, total_loss/examples = 3.02954327341
03-01-2018:00:49:34,561 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1020, total_loss/examples = 3.02294508764
03-01-2018:00:49:40,902 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1030, total_loss/examples = 3.01782035354
03-01-2018:00:49:46,957 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1040, total_loss/examples = 3.01082776694
03-01-2018:00:49:53,62 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1050, total_loss/examples = 3.00535666795
03-01-2018:00:49:59,295 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1060, total_loss/examples = 3.00040084331
03-01-2018:00:50:05,329 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1070, total_loss/examples = 2.99078497043
03-01-2018:00:50:11,714 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1080, total_loss/examples = 2.98713644283
03-01-2018:00:50:17,725 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1090, total_loss/examples = 2.98305853062
03-01-2018:00:50:23,581 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1100, total_loss/examples = 2.9808400734
03-01-2018:00:50:28,803 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1110, total_loss/examples = 2.97457062651
03-01-2018:00:50:35,681 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1120, total_loss/examples = 2.9716542235
03-01-2018:00:50:41,643 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1130, total_loss/examples = 2.96428193728
03-01-2018:00:50:47,805 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1140, total_loss/examples = 2.95889948618
03-01-2018:00:50:55,50 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1150, total_loss/examples = 2.95404343383
03-01-2018:00:51:01,156 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1160, total_loss/examples = 2.94694395285
03-01-2018:00:51:07,545 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1170, total_loss/examples = 2.9433680448
03-01-2018:00:51:13,682 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1180, total_loss/examples = 2.93813149441
03-01-2018:00:51:19,758 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1190, total_loss/examples = 2.93101239635
03-01-2018:00:51:26,219 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1200, total_loss/examples = 2.92663514763
03-01-2018:00:51:32,277 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1210, total_loss/examples = 2.92076000972
03-01-2018:00:51:38,185 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1220, total_loss/examples = 2.91643441026
03-01-2018:00:51:44,235 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1230, total_loss/examples = 2.91145541647
03-01-2018:00:51:50,471 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1240, total_loss/examples = 2.90720594976
03-01-2018:00:51:56,613 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1250, total_loss/examples = 2.90130176361
03-01-2018:00:52:01,811 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1260, total_loss/examples = 2.89686841704
03-01-2018:00:52:07,686 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1270, total_loss/examples = 2.89077014111
03-01-2018:00:52:13,797 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1280, total_loss/examples = 2.88410647459
03-01-2018:00:52:20,70 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1290, total_loss/examples = 2.87929365443
03-01-2018:00:52:26,313 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1300, total_loss/examples = 2.87468618077
03-01-2018:00:52:32,328 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1310, total_loss/examples = 2.8708571873
03-01-2018:00:52:38,239 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1320, total_loss/examples = 2.86663861495
03-01-2018:00:52:44,659 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1330, total_loss/examples = 2.86177574388
03-01-2018:00:52:50,620 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1340, total_loss/examples = 2.85829857425
03-01-2018:00:52:56,819 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1350, total_loss/examples = 2.85547878626
03-01-2018:00:53:03,20 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1360, total_loss/examples = 2.85051000696
03-01-2018:00:53:09,42 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1370, total_loss/examples = 2.84607672265
03-01-2018:00:53:15,238 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1380, total_loss/examples = 2.84122851989
03-01-2018:00:53:21,303 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1390, total_loss/examples = 2.83772947924
03-01-2018:00:53:26,214 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1400, total_loss/examples = 2.83303712122
03-01-2018:00:53:32,559 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1410, total_loss/examples = 2.82849556592
03-01-2018:00:53:38,626 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1420, total_loss/examples = 2.82422431567
03-01-2018:00:53:44,536 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1430, total_loss/examples = 2.81897557548
03-01-2018:00:53:50,625 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1440, total_loss/examples = 2.81399322648
03-01-2018:00:53:56,913 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1450, total_loss/examples = 2.80835401404
03-01-2018:00:54:03,25 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1460, total_loss/examples = 2.80396556503
03-01-2018:00:54:08,914 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1470, total_loss/examples = 2.79955599245
03-01-2018:00:54:15,33 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1480, total_loss/examples = 2.79476768958
03-01-2018:00:54:20,747 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1490, total_loss/examples = 2.78974733114
03-01-2018:00:54:26,767 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1500, total_loss/examples = 2.78610444077
03-01-2018:00:54:32,752 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1510, total_loss/examples = 2.78286709334
03-01-2018:00:54:38,797 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1520, total_loss/examples = 2.77800697531
03-01-2018:00:54:45,32 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1530, total_loss/examples = 2.77304198958
03-01-2018:00:54:51,502 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1540, total_loss/examples = 2.76774949961
03-01-2018:00:54:57,482 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1550, total_loss/examples = 2.76289571792
03-01-2018:00:55:03,569 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1560, total_loss/examples = 2.75856491307
03-01-2018:00:55:08,836 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1570, total_loss/examples = 2.7539063186
03-01-2018:00:55:14,791 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1580, total_loss/examples = 2.7493633436
03-01-2018:00:55:20,608 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1590, total_loss/examples = 2.7428631327
03-01-2018:00:55:26,621 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1600, total_loss/examples = 2.73873640469
03-01-2018:00:55:32,764 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1610, total_loss/examples = 2.73536516272
03-01-2018:00:55:38,795 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1620, total_loss/examples = 2.73211150762
03-01-2018:00:55:45,438 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1630, total_loss/examples = 2.72886184583
03-01-2018:00:55:51,560 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1640, total_loss/examples = 2.72476967873
03-01-2018:00:55:57,635 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1650, total_loss/examples = 2.71970860161
03-01-2018:00:56:02,614 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1660, total_loss/examples = 2.71414126085
03-01-2018:00:56:08,589 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1670, total_loss/examples = 2.70856970217
03-01-2018:00:56:14,606 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1680, total_loss/examples = 2.70408018227
03-01-2018:00:56:20,697 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1690, total_loss/examples = 2.69989827023
03-01-2018:00:56:26,765 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1700, total_loss/examples = 2.69685935168
03-01-2018:00:56:32,928 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1710, total_loss/examples = 2.69376205032
03-01-2018:00:56:39,300 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1720, total_loss/examples = 2.68899665711
03-01-2018:00:56:45,457 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1730, total_loss/examples = 2.68615328474
03-01-2018:00:56:51,409 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1740, total_loss/examples = 2.68336457209
03-01-2018:00:56:57,423 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1750, total_loss/examples = 2.67934543688
03-01-2018:00:57:02,543 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1760, total_loss/examples = 2.67570049325
03-01-2018:00:57:08,867 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1770, total_loss/examples = 2.6713603253
03-01-2018:00:57:14,952 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1780, total_loss/examples = 2.6675701223
03-01-2018:00:57:21,222 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1790, total_loss/examples = 2.66416651387
03-01-2018:00:57:27,304 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1800, total_loss/examples = 2.66161851106
03-01-2018:00:57:33,351 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1810, total_loss/examples = 2.65779202666
03-01-2018:00:57:39,547 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1820, total_loss/examples = 2.65328643808
03-01-2018:00:57:45,532 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1830, total_loss/examples = 2.64971280157
03-01-2018:00:57:51,769 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1840, total_loss/examples = 2.64799192119
03-01-2018:00:57:57,719 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1850, total_loss/examples = 2.6453448525
03-01-2018:00:58:03,771 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1860, total_loss/examples = 2.64099761512
03-01-2018:00:58:09,699 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1870, total_loss/examples = 2.63665406332
03-01-2018:00:58:15,688 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1880, total_loss/examples = 2.63325232644
03-01-2018:00:58:20,885 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1890, total_loss/examples = 2.62984358029
03-01-2018:00:58:27,8 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1900, total_loss/examples = 2.62589463091
03-01-2018:00:58:33,131 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1910, total_loss/examples = 2.6223742033
03-01-2018:00:58:39,215 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1920, total_loss/examples = 2.6191026789
03-01-2018:00:58:45,279 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1930, total_loss/examples = 2.61527825535
03-01-2018:00:58:51,331 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1940, total_loss/examples = 2.61045275986
03-01-2018:00:58:58,109 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1950, total_loss/examples = 2.60620094974
03-01-2018:00:59:04,3 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1960, total_loss/examples = 2.60205968505
03-01-2018:00:59:09,831 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1970, total_loss/examples = 2.59938512811
03-01-2018:00:59:15,993 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1980, total_loss/examples = 2.59572700561
03-01-2018:00:59:22,185 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 1990, total_loss/examples = 2.59167937835
03-01-2018:00:59:28,525 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2000, total_loss/examples = 2.58833813328
03-01-2018:00:59:33,657 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2010, total_loss/examples = 2.58463238016
03-01-2018:00:59:39,565 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2020, total_loss/examples = 2.58163076872
03-01-2018:00:59:45,334 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2030, total_loss/examples = 2.57848521355
03-01-2018:00:59:51,666 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2040, total_loss/examples = 2.57423685503
03-01-2018:00:59:58,189 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2050, total_loss/examples = 2.57148846564
03-01-2018:01:00:03,294 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2060, total_loss/examples = 2.56735233402
03-01-2018:01:00:09,855 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2070, total_loss/examples = 2.56381267991
03-01-2018:01:00:16,21 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2080, total_loss/examples = 2.56036796288
03-01-2018:01:00:22,31 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2090, total_loss/examples = 2.5566595252
03-01-2018:01:00:28,233 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2100, total_loss/examples = 2.5521815544
03-01-2018:01:00:34,462 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2110, total_loss/examples = 2.54905814508
03-01-2018:01:00:40,790 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2120, total_loss/examples = 2.54453461514
03-01-2018:01:00:47,16 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2130, total_loss/examples = 2.54007168683
03-01-2018:01:00:53,143 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2140, total_loss/examples = 2.5368266558
03-01-2018:01:00:59,273 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2150, total_loss/examples = 2.53391066954
03-01-2018:01:01:05,429 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2160, total_loss/examples = 2.53163919544
03-01-2018:01:01:11,408 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2170, total_loss/examples = 2.52913385177
03-01-2018:01:01:17,456 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2180, total_loss/examples = 2.52607394888
03-01-2018:01:01:22,975 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2190, total_loss/examples = 2.52198110536
03-01-2018:01:01:29,22 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2200, total_loss/examples = 2.51851909684
03-01-2018:01:01:35,461 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2210, total_loss/examples = 2.51486358398
03-01-2018:01:01:41,671 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2220, total_loss/examples = 2.51149977716
03-01-2018:01:01:47,861 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2230, total_loss/examples = 2.50855314462
03-01-2018:01:01:54,11 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2240, total_loss/examples = 2.5060121974
03-01-2018:01:02:00,365 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2250, total_loss/examples = 2.50284359091
03-01-2018:01:02:06,849 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2260, total_loss/examples = 2.49937162551
03-01-2018:01:02:12,920 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2270, total_loss/examples = 2.49653016736
03-01-2018:01:02:19,360 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2280, total_loss/examples = 2.49383296493
03-01-2018:01:02:24,206 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2290, total_loss/examples = 2.49137406183
03-01-2018:01:02:30,322 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2300, total_loss/examples = 2.48741076841
03-01-2018:01:02:36,404 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2310, total_loss/examples = 2.48380967501
03-01-2018:01:02:42,347 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2320, total_loss/examples = 2.48078425988
03-01-2018:01:02:48,656 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2330, total_loss/examples = 2.47770324762
03-01-2018:01:02:54,905 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2340, total_loss/examples = 2.47398917159
03-01-2018:01:03:00,994 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2350, total_loss/examples = 2.47087552683
03-01-2018:01:03:06,906 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2360, total_loss/examples = 2.46749603231
03-01-2018:01:03:13,192 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2370, total_loss/examples = 2.46479906907
03-01-2018:01:03:19,530 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2380, total_loss/examples = 2.46121856003
03-01-2018:01:03:24,894 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2390, total_loss/examples = 2.45859843109
03-01-2018:01:03:31,288 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2400, total_loss/examples = 2.45595092262
03-01-2018:01:03:37,458 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2410, total_loss/examples = 2.45294739171
03-01-2018:01:03:43,649 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2420, total_loss/examples = 2.45012966994
03-01-2018:01:03:49,556 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2430, total_loss/examples = 2.4471481731
03-01-2018:01:03:55,509 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2440, total_loss/examples = 2.44326643424
03-01-2018:01:04:01,732 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2450, total_loss/examples = 2.44043469794
03-01-2018:01:04:07,910 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2460, total_loss/examples = 2.43869767892
03-01-2018:01:04:13,925 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2470, total_loss/examples = 2.43500704627
03-01-2018:01:04:20,547 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2480, total_loss/examples = 2.43263588164
03-01-2018:01:04:26,34 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2490, total_loss/examples = 2.43011662439
03-01-2018:01:04:32,199 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2500, total_loss/examples = 2.42784289463
03-01-2018:01:04:38,217 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2510, total_loss/examples = 2.4251209759
03-01-2018:01:04:44,772 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2520, total_loss/examples = 2.42183339449
03-01-2018:01:04:51,4 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2530, total_loss/examples = 2.4175914838
03-01-2018:01:04:57,626 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2540, total_loss/examples = 2.41534957854
03-01-2018:01:05:04,864 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2550, total_loss/examples = 2.41281143351
03-01-2018:01:05:12,274 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2560, total_loss/examples = 2.40979876536
03-01-2018:01:05:18,430 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2570, total_loss/examples = 2.40723905277
03-01-2018:01:05:24,351 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2580, total_loss/examples = 2.40548723375
03-01-2018:01:05:30,342 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2590, total_loss/examples = 2.40243127996
03-01-2018:01:05:35,468 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2600, total_loss/examples = 2.39961695811
03-01-2018:01:05:41,530 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2610, total_loss/examples = 2.39690479163
03-01-2018:01:05:47,332 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2620, total_loss/examples = 2.39445114916
03-01-2018:01:05:53,638 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2630, total_loss/examples = 2.39121212965
03-01-2018:01:05:59,727 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2640, total_loss/examples = 2.38813431145
03-01-2018:01:06:05,865 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2650, total_loss/examples = 2.38561660326
03-01-2018:01:06:12,58 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2660, total_loss/examples = 2.38374011293
03-01-2018:01:06:18,160 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2670, total_loss/examples = 2.38079975406
03-01-2018:01:06:23,53 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2680, total_loss/examples = 2.3778830001
03-01-2018:01:06:29,51 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2690, total_loss/examples = 2.37437637893
03-01-2018:01:06:35,507 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2700, total_loss/examples = 2.37198848898
03-01-2018:01:06:41,576 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2710, total_loss/examples = 2.36833057908
03-01-2018:01:06:47,518 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2720, total_loss/examples = 2.36552137653
03-01-2018:01:06:53,495 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2730, total_loss/examples = 2.3627793866
03-01-2018:01:06:59,616 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2740, total_loss/examples = 2.35932744985
03-01-2018:01:07:05,728 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2750, total_loss/examples = 2.35715314779
03-01-2018:01:07:11,615 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2760, total_loss/examples = 2.35420034157
03-01-2018:01:07:17,633 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2770, total_loss/examples = 2.35103890488
03-01-2018:01:07:22,762 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2780, total_loss/examples = 2.3488842861
03-01-2018:01:07:28,725 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2790, total_loss/examples = 2.34725837306
03-01-2018:01:07:34,901 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2800, total_loss/examples = 2.34411211939
03-01-2018:01:07:40,915 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2810, total_loss/examples = 2.34146830485
03-01-2018:01:07:46,863 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2820, total_loss/examples = 2.3390430916
03-01-2018:01:07:52,925 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2830, total_loss/examples = 2.33648624504
03-01-2018:01:07:58,914 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2840, total_loss/examples = 2.3345542222
03-01-2018:01:08:05,179 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2850, total_loss/examples = 2.33213525981
03-01-2018:01:08:11,470 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2860, total_loss/examples = 2.32912282532
03-01-2018:01:08:17,404 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2870, total_loss/examples = 2.32621795938
03-01-2018:01:08:23,546 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2880, total_loss/examples = 2.32282746523
03-01-2018:01:08:29,460 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2890, total_loss/examples = 2.31943514484
03-01-2018:01:08:35,406 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2900, total_loss/examples = 2.31747933956
03-01-2018:01:08:41,380 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2910, total_loss/examples = 2.31516784544
03-01-2018:01:08:47,621 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2920, total_loss/examples = 2.31244446869
03-01-2018:01:08:53,788 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2930, total_loss/examples = 2.310564351
03-01-2018:01:08:58,827 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2940, total_loss/examples = 2.30894581068
03-01-2018:01:09:04,903 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2950, total_loss/examples = 2.30622374242
03-01-2018:01:09:11,159 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2960, total_loss/examples = 2.30313713953
03-01-2018:01:09:17,91 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2970, total_loss/examples = 2.30068560515
03-01-2018:01:09:23,256 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2980, total_loss/examples = 2.29880342928
03-01-2018:01:09:29,354 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 2990, total_loss/examples = 2.29558397983
03-01-2018:01:09:35,417 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3000, total_loss/examples = 2.29401335358
03-01-2018:01:09:41,453 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3010, total_loss/examples = 2.29147166873
03-01-2018:01:09:47,796 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3020, total_loss/examples = 2.28915327326
03-01-2018:01:09:53,942 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3030, total_loss/examples = 2.28705756071
03-01-2018:01:10:00,73 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3040, total_loss/examples = 2.28469190906
03-01-2018:01:10:06,174 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3050, total_loss/examples = 2.28285563328
03-01-2018:01:10:11,517 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3060, total_loss/examples = 2.28114214256
03-01-2018:01:10:17,565 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3070, total_loss/examples = 2.27885293099
03-01-2018:01:10:23,826 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3080, total_loss/examples = 2.2769243119
03-01-2018:01:10:30,760 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3090, total_loss/examples = 2.27483994712
03-01-2018:01:10:36,691 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3100, total_loss/examples = 2.27222411141
03-01-2018:01:10:42,771 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3110, total_loss/examples = 2.27021375945
03-01-2018:01:10:48,924 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3120, total_loss/examples = 2.2681226311
03-01-2018:01:10:55,394 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3130, total_loss/examples = 2.26613695718
03-01-2018:01:11:01,391 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3140, total_loss/examples = 2.26375539073
03-01-2018:01:11:07,412 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3150, total_loss/examples = 2.26163081779
03-01-2018:01:11:13,633 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3160, total_loss/examples = 2.25939951792
03-01-2018:01:11:18,604 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3170, total_loss/examples = 2.25677492757
03-01-2018:01:11:25,508 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3180, total_loss/examples = 2.25480940467
03-01-2018:01:11:31,591 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3190, total_loss/examples = 2.25277925367
03-01-2018:01:11:37,698 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3200, total_loss/examples = 2.25066615916
03-01-2018:01:11:44,230 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3210, total_loss/examples = 2.24895360358
03-01-2018:01:11:50,249 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3220, total_loss/examples = 2.246643959
03-01-2018:01:11:56,450 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3230, total_loss/examples = 2.244711593
03-01-2018:01:12:02,424 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3240, total_loss/examples = 2.2422022693
03-01-2018:01:12:08,478 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3250, total_loss/examples = 2.24076777669
03-01-2018:01:12:14,570 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3260, total_loss/examples = 2.23865685511
03-01-2018:01:12:20,793 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3270, total_loss/examples = 2.23604610467
03-01-2018:01:12:27,29 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3280, total_loss/examples = 2.23419250567
03-01-2018:01:12:33,161 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3290, total_loss/examples = 2.23190381153
03-01-2018:01:12:39,200 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3300, total_loss/examples = 2.23015953118
03-01-2018:01:12:44,501 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3310, total_loss/examples = 2.22764228682
03-01-2018:01:12:50,238 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3320, total_loss/examples = 2.2251643583
03-01-2018:01:12:56,201 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3330, total_loss/examples = 2.22328076786
03-01-2018:01:13:02,272 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3340, total_loss/examples = 2.22072855192
03-01-2018:01:13:08,335 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3350, total_loss/examples = 2.2193075403
03-01-2018:01:13:14,728 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3360, total_loss/examples = 2.2175641375
03-01-2018:01:13:20,699 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3370, total_loss/examples = 2.21571711235
03-01-2018:01:13:21,813 INFO     [ASReaderTrainer.py:191] Starting to calculate accuracy
03-01-2018:01:13:21,813 INFO     [ASReaderTrainer.py:185] Starting to predict
03-01-2018:01:15:46,588 INFO     [ASReaderTrainer.py:146] valid_accuracy = 0.553490708187
03-01-2018:01:15:46,588 INFO     [ASReaderTrainer.py:149] previous valid accuracy = 0.225514816675, current valid accuracy = 0.553490708187
03-01-2018:01:15:46,599 INFO     [ASReaderTrainer.py:215] Saving model in file: /home/shantanu/PycharmProjects/attentionSum/generated_data/model_save.txt
03-01-2018:01:15:49,481 INFO     [ASReaderTrainer.py:218] Saving model args in file: /home/shantanu/PycharmProjects/attentionSum/generated_data/model_args_save.txt
03-01-2018:01:15:49,481 INFO     [ASReaderTrainer.py:221] Done saving model and model args
03-01-2018:01:15:54,391 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3380, total_loss/examples = 2.21388095039
03-01-2018:01:16:00,162 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3390, total_loss/examples = 2.21123916264
03-01-2018:01:16:05,981 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3400, total_loss/examples = 2.20927468167
03-01-2018:01:16:12,59 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3410, total_loss/examples = 2.20734346018
03-01-2018:01:16:18,186 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3420, total_loss/examples = 2.20530144651
03-01-2018:01:16:24,229 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3430, total_loss/examples = 2.20296236078
03-01-2018:01:16:29,308 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3440, total_loss/examples = 2.20065143892
03-01-2018:01:16:35,310 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3450, total_loss/examples = 2.19859451807
03-01-2018:01:16:41,228 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3460, total_loss/examples = 2.19635191893
03-01-2018:01:16:47,203 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3470, total_loss/examples = 2.19432112503
03-01-2018:01:16:53,161 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3480, total_loss/examples = 2.19182041254
03-01-2018:01:16:59,84 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3490, total_loss/examples = 2.18945437438
03-01-2018:01:17:05,204 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3500, total_loss/examples = 2.18692362259
03-01-2018:01:17:11,496 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3510, total_loss/examples = 2.18455677798
03-01-2018:01:17:17,564 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3520, total_loss/examples = 2.18213644604
03-01-2018:01:17:23,533 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3530, total_loss/examples = 2.18075811321
03-01-2018:01:17:28,543 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3540, total_loss/examples = 2.17901994205
03-01-2018:01:17:34,705 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3550, total_loss/examples = 2.17738522459
03-01-2018:01:17:40,750 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3560, total_loss/examples = 2.17531721186
03-01-2018:01:17:46,791 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3570, total_loss/examples = 2.17293579451
03-01-2018:01:17:52,696 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3580, total_loss/examples = 2.17133502874
03-01-2018:01:17:58,922 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3590, total_loss/examples = 2.16900103966
03-01-2018:01:18:04,949 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3600, total_loss/examples = 2.16637237226
03-01-2018:01:18:11,164 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3610, total_loss/examples = 2.16452295318
03-01-2018:01:18:16,646 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3620, total_loss/examples = 2.16244656816
03-01-2018:01:18:23,639 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3630, total_loss/examples = 2.16066876185
03-01-2018:01:18:28,687 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3640, total_loss/examples = 2.15923494851
03-01-2018:01:18:34,623 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3650, total_loss/examples = 2.15711372429
03-01-2018:01:18:40,666 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3660, total_loss/examples = 2.15478529896
03-01-2018:01:18:46,759 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3670, total_loss/examples = 2.15313734356
03-01-2018:01:18:53,265 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3680, total_loss/examples = 2.15137573413
03-01-2018:01:18:59,339 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3690, total_loss/examples = 2.1501667669
03-01-2018:01:19:05,584 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3700, total_loss/examples = 2.14805624865
03-01-2018:01:19:11,954 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3710, total_loss/examples = 2.14611259639
03-01-2018:01:19:18,331 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3720, total_loss/examples = 2.14422055216
03-01-2018:01:19:24,281 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3730, total_loss/examples = 2.14167562774
03-01-2018:01:19:30,327 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3740, total_loss/examples = 2.13961007379
03-01-2018:01:19:36,354 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3750, total_loss/examples = 2.13734722381
03-01-2018:01:19:42,338 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3760, total_loss/examples = 2.13581558014
03-01-2018:01:19:48,343 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3770, total_loss/examples = 2.13375341057
03-01-2018:01:19:54,509 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3780, total_loss/examples = 2.13202315844
03-01-2018:01:19:59,616 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3790, total_loss/examples = 2.12975345483
03-01-2018:01:20:05,899 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3800, total_loss/examples = 2.12782708213
03-01-2018:01:20:11,756 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3810, total_loss/examples = 2.12638005907
03-01-2018:01:20:17,678 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3820, total_loss/examples = 2.12436766478
03-01-2018:01:20:23,797 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3830, total_loss/examples = 2.12275178792
03-01-2018:01:20:30,483 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3840, total_loss/examples = 2.12095540991
03-01-2018:01:20:36,719 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3850, total_loss/examples = 2.11924014025
03-01-2018:01:20:42,668 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3860, total_loss/examples = 2.11742215598
03-01-2018:01:20:48,551 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3870, total_loss/examples = 2.11551550306
03-01-2018:01:20:54,598 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3880, total_loss/examples = 2.11384354413
03-01-2018:01:20:59,668 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3890, total_loss/examples = 2.11226476851
03-01-2018:01:21:05,681 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3900, total_loss/examples = 2.11054881895
03-01-2018:01:21:11,853 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3910, total_loss/examples = 2.10879816287
03-01-2018:01:21:18,68 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3920, total_loss/examples = 2.10713627437
03-01-2018:01:21:24,319 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3930, total_loss/examples = 2.1055516124
03-01-2018:01:21:30,418 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3940, total_loss/examples = 2.10410059856
03-01-2018:01:21:36,487 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3950, total_loss/examples = 2.10253054374
03-01-2018:01:21:42,416 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3960, total_loss/examples = 2.10057551318
03-01-2018:01:21:48,327 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3970, total_loss/examples = 2.09875814667
03-01-2018:01:21:53,399 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3980, total_loss/examples = 2.09708162
03-01-2018:01:21:59,857 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 3990, total_loss/examples = 2.09495004983
03-01-2018:01:22:05,855 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4000, total_loss/examples = 2.09379573232
03-01-2018:01:22:11,737 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4010, total_loss/examples = 2.09168689048
03-01-2018:01:22:17,771 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4020, total_loss/examples = 2.08968350998
03-01-2018:01:22:23,746 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4030, total_loss/examples = 2.08841858175
03-01-2018:01:22:29,952 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4040, total_loss/examples = 2.08700073515
03-01-2018:01:22:36,157 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4050, total_loss/examples = 2.08549524283
03-01-2018:01:22:41,249 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4060, total_loss/examples = 2.08379525713
03-01-2018:01:22:47,160 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4070, total_loss/examples = 2.081797614
03-01-2018:01:22:53,194 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4080, total_loss/examples = 2.08041191636
03-01-2018:01:22:59,69 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4090, total_loss/examples = 2.07857750642
03-01-2018:01:23:05,400 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4100, total_loss/examples = 2.07699439293
03-01-2018:01:23:11,594 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4110, total_loss/examples = 2.07521012655
03-01-2018:01:23:17,654 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4120, total_loss/examples = 2.0743199306
03-01-2018:01:23:23,502 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4130, total_loss/examples = 2.07265282008
03-01-2018:01:23:29,513 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4140, total_loss/examples = 2.07112167324
03-01-2018:01:23:35,570 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4150, total_loss/examples = 2.06931722403
03-01-2018:01:23:41,737 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4160, total_loss/examples = 2.06777335139
03-01-2018:01:23:47,560 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4170, total_loss/examples = 2.0657817871
03-01-2018:01:23:54,442 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4180, total_loss/examples = 2.06480866232
03-01-2018:01:24:00,897 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4190, total_loss/examples = 2.06309352299
03-01-2018:01:24:06,277 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4200, total_loss/examples = 2.06182366117
03-01-2018:01:24:12,441 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4210, total_loss/examples = 2.060484156
03-01-2018:01:24:18,454 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4220, total_loss/examples = 2.05908473425
03-01-2018:01:24:24,610 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4230, total_loss/examples = 2.05750446919
03-01-2018:01:24:30,571 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4240, total_loss/examples = 2.05556650191
03-01-2018:01:24:36,771 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4250, total_loss/examples = 2.05404532456
03-01-2018:01:24:42,749 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4260, total_loss/examples = 2.05302463076
03-01-2018:01:24:48,717 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4270, total_loss/examples = 2.05104115325
03-01-2018:01:24:54,640 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4280, total_loss/examples = 2.04911222938
03-01-2018:01:25:00,656 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4290, total_loss/examples = 2.04718472981
03-01-2018:01:25:06,846 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4300, total_loss/examples = 2.04528346569
03-01-2018:01:25:12,898 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4310, total_loss/examples = 2.04379625105
03-01-2018:01:25:18,904 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4320, total_loss/examples = 2.04230663982
03-01-2018:01:25:24,978 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4330, total_loss/examples = 2.04082313031
03-01-2018:01:25:31,82 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4340, total_loss/examples = 2.0394110187
03-01-2018:01:25:36,200 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4350, total_loss/examples = 2.0380395457
03-01-2018:01:25:42,277 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4360, total_loss/examples = 2.03669996598
03-01-2018:01:25:48,768 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4370, total_loss/examples = 2.03508523807
03-01-2018:01:25:54,759 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4380, total_loss/examples = 2.03339510517
03-01-2018:01:26:00,932 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4390, total_loss/examples = 2.03206845354
03-01-2018:01:26:07,214 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4400, total_loss/examples = 2.03107043489
03-01-2018:01:26:13,434 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4410, total_loss/examples = 2.02950478802
03-01-2018:01:26:19,575 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4420, total_loss/examples = 2.02765472437
03-01-2018:01:26:25,481 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4430, total_loss/examples = 2.02618648062
03-01-2018:01:26:31,578 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4440, total_loss/examples = 2.02432159217
03-01-2018:01:26:36,777 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4450, total_loss/examples = 2.02304794588
03-01-2018:01:26:42,775 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4460, total_loss/examples = 2.02175168042
03-01-2018:01:26:48,924 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4470, total_loss/examples = 2.02025128245
03-01-2018:01:26:54,801 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4480, total_loss/examples = 2.01891711357
03-01-2018:01:27:01,5 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4490, total_loss/examples = 2.01731748968
03-01-2018:01:27:06,925 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4500, total_loss/examples = 2.01580689704
03-01-2018:01:27:13,2 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4510, total_loss/examples = 2.01479539476
03-01-2018:01:27:19,194 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4520, total_loss/examples = 2.01352341079
03-01-2018:01:27:25,115 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4530, total_loss/examples = 2.01212877211
03-01-2018:01:27:30,755 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4540, total_loss/examples = 2.01056641666
03-01-2018:01:27:36,762 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4550, total_loss/examples = 2.0089056053
03-01-2018:01:27:42,972 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4560, total_loss/examples = 2.00775649052
03-01-2018:01:27:48,919 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4570, total_loss/examples = 2.00607162991
03-01-2018:01:27:54,985 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4580, total_loss/examples = 2.00441108066
03-01-2018:01:28:00,933 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4590, total_loss/examples = 2.00352184216
03-01-2018:01:28:07,241 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4600, total_loss/examples = 2.00182466452
03-01-2018:01:28:13,361 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4610, total_loss/examples = 2.00017473343
03-01-2018:01:28:19,480 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4620, total_loss/examples = 1.99880992314
03-01-2018:01:28:24,608 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4630, total_loss/examples = 1.9974773691
03-01-2018:01:28:30,551 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4640, total_loss/examples = 1.99567332134
03-01-2018:01:28:36,670 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4650, total_loss/examples = 1.99412917065
03-01-2018:01:28:43,148 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4660, total_loss/examples = 1.9925729293
03-01-2018:01:28:49,831 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4670, total_loss/examples = 1.99158521585
03-01-2018:01:28:55,862 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4680, total_loss/examples = 1.99054391211
03-01-2018:01:29:01,806 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4690, total_loss/examples = 1.98915393152
03-01-2018:01:29:07,742 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4700, total_loss/examples = 1.98729399508
03-01-2018:01:29:13,623 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4710, total_loss/examples = 1.98586203991
03-01-2018:01:29:19,580 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4720, total_loss/examples = 1.9840474972
03-01-2018:01:29:24,737 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4730, total_loss/examples = 1.98295663562
03-01-2018:01:29:30,851 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4740, total_loss/examples = 1.98241914214
03-01-2018:01:29:37,42 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4750, total_loss/examples = 1.98090355987
03-01-2018:01:29:43,250 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4760, total_loss/examples = 1.9795216988
03-01-2018:01:29:49,260 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4770, total_loss/examples = 1.97831346132
03-01-2018:01:29:55,515 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4780, total_loss/examples = 1.97756846718
03-01-2018:01:30:02,75 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4790, total_loss/examples = 1.97620616173
03-01-2018:01:30:08,293 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4800, total_loss/examples = 1.97487411539
03-01-2018:01:30:14,748 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4810, total_loss/examples = 1.9732733811
03-01-2018:01:30:20,792 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4820, total_loss/examples = 1.9722203398
03-01-2018:01:30:27,192 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4830, total_loss/examples = 1.97113974672
03-01-2018:01:30:33,509 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4840, total_loss/examples = 1.96940015907
03-01-2018:01:30:39,950 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4850, total_loss/examples = 1.96788859874
03-01-2018:01:30:46,154 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4860, total_loss/examples = 1.96667948044
03-01-2018:01:30:52,879 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4870, total_loss/examples = 1.96546823371
03-01-2018:01:30:58,999 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4880, total_loss/examples = 1.96452419622
03-01-2018:01:31:05,307 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4890, total_loss/examples = 1.96358034843
03-01-2018:01:31:11,404 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4900, total_loss/examples = 1.96208994889
03-01-2018:01:31:16,369 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4910, total_loss/examples = 1.96085020701
03-01-2018:01:31:22,515 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4920, total_loss/examples = 1.959589433
03-01-2018:01:31:28,532 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4930, total_loss/examples = 1.95858342184
03-01-2018:01:31:34,489 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4940, total_loss/examples = 1.95766634081
03-01-2018:01:31:40,589 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4950, total_loss/examples = 1.95605647664
03-01-2018:01:31:46,615 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4960, total_loss/examples = 1.95461428048
03-01-2018:01:31:52,818 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4970, total_loss/examples = 1.95346225022
03-01-2018:01:31:59,174 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4980, total_loss/examples = 1.95205181685
03-01-2018:01:32:05,435 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 4990, total_loss/examples = 1.95059712062
03-01-2018:01:32:11,677 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5000, total_loss/examples = 1.94988422695
03-01-2018:01:32:17,904 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5010, total_loss/examples = 1.94869006811
03-01-2018:01:32:23,74 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5020, total_loss/examples = 1.94785281943
03-01-2018:01:32:29,173 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5030, total_loss/examples = 1.94632190746
03-01-2018:01:32:34,896 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5040, total_loss/examples = 1.94544546635
03-01-2018:01:32:40,855 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5050, total_loss/examples = 1.94447284317
03-01-2018:01:32:46,846 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5060, total_loss/examples = 1.94315937515
03-01-2018:01:32:53,51 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5070, total_loss/examples = 1.94154520617
03-01-2018:01:32:59,78 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5080, total_loss/examples = 1.94029144964
03-01-2018:01:33:05,116 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5090, total_loss/examples = 1.93911999943
03-01-2018:01:33:11,166 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5100, total_loss/examples = 1.9377293149
03-01-2018:01:33:16,475 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5110, total_loss/examples = 1.93608936685
03-01-2018:01:33:22,732 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5120, total_loss/examples = 1.93503609985
03-01-2018:01:33:28,866 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5130, total_loss/examples = 1.93373888196
03-01-2018:01:33:34,991 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5140, total_loss/examples = 1.93236434769
03-01-2018:01:33:41,126 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5150, total_loss/examples = 1.93119988933
03-01-2018:01:33:47,253 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5160, total_loss/examples = 1.93032390602
03-01-2018:01:33:53,361 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5170, total_loss/examples = 1.92906031162
03-01-2018:01:33:59,466 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5180, total_loss/examples = 1.92770924773
03-01-2018:01:34:05,494 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5190, total_loss/examples = 1.92662473862
03-01-2018:01:34:10,630 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5200, total_loss/examples = 1.92543240234
03-01-2018:01:34:17,535 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5210, total_loss/examples = 1.92443252459
03-01-2018:01:34:23,990 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5220, total_loss/examples = 1.92303854788
03-01-2018:01:34:29,878 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5230, total_loss/examples = 1.92210187592
03-01-2018:01:34:36,48 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5240, total_loss/examples = 1.92061060616
03-01-2018:01:34:42,35 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5250, total_loss/examples = 1.91913662565
03-01-2018:01:34:48,293 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5260, total_loss/examples = 1.91809357447
03-01-2018:01:34:54,369 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5270, total_loss/examples = 1.91676972099
03-01-2018:01:35:00,315 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5280, total_loss/examples = 1.91534540582
03-01-2018:01:35:06,438 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5290, total_loss/examples = 1.91454046548
03-01-2018:01:35:12,735 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5300, total_loss/examples = 1.91333550785
03-01-2018:01:35:20,838 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5310, total_loss/examples = 1.91244287559
03-01-2018:01:35:26,545 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5320, total_loss/examples = 1.91103771723
03-01-2018:01:35:32,638 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5330, total_loss/examples = 1.90966973115
03-01-2018:01:35:38,679 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5340, total_loss/examples = 1.90820801702
03-01-2018:01:35:44,535 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5350, total_loss/examples = 1.90716225516
03-01-2018:01:35:50,719 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5360, total_loss/examples = 1.90582868015
03-01-2018:01:35:56,806 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5370, total_loss/examples = 1.90462930438
03-01-2018:01:36:03,310 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5380, total_loss/examples = 1.90368670859
03-01-2018:01:36:09,299 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5390, total_loss/examples = 1.90220804126
03-01-2018:01:36:14,515 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5400, total_loss/examples = 1.90129277464
03-01-2018:01:36:20,577 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5410, total_loss/examples = 1.90006276426
03-01-2018:01:36:27,221 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5420, total_loss/examples = 1.8991431867
03-01-2018:01:36:33,288 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5430, total_loss/examples = 1.89816140416
03-01-2018:01:36:39,358 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5440, total_loss/examples = 1.89748025473
03-01-2018:01:36:45,507 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5450, total_loss/examples = 1.8961584572
03-01-2018:01:36:51,510 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5460, total_loss/examples = 1.89503075506
03-01-2018:01:36:57,520 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5470, total_loss/examples = 1.893694506
03-01-2018:01:37:03,693 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5480, total_loss/examples = 1.89234620068
03-01-2018:01:37:09,589 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5490, total_loss/examples = 1.89127084122
03-01-2018:01:37:14,788 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5500, total_loss/examples = 1.88995607198
03-01-2018:01:37:22,453 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5510, total_loss/examples = 1.88873664992
03-01-2018:01:37:28,677 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5520, total_loss/examples = 1.88790935849
03-01-2018:01:37:33,704 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5530, total_loss/examples = 1.88687452507
03-01-2018:01:37:39,645 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5540, total_loss/examples = 1.88604554228
03-01-2018:01:37:45,619 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5550, total_loss/examples = 1.88487367258
03-01-2018:01:37:51,866 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5560, total_loss/examples = 1.88353747372
03-01-2018:01:37:57,950 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5570, total_loss/examples = 1.88259277403
03-01-2018:01:38:04,124 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5580, total_loss/examples = 1.88149767267
03-01-2018:01:38:10,800 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5590, total_loss/examples = 1.88067073272
03-01-2018:01:38:16,953 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5600, total_loss/examples = 1.87971652751
03-01-2018:01:38:22,975 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5610, total_loss/examples = 1.87888258507
03-01-2018:01:38:29,677 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5620, total_loss/examples = 1.87813075345
03-01-2018:01:38:35,731 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5630, total_loss/examples = 1.87713298916
03-01-2018:01:38:40,722 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5640, total_loss/examples = 1.87620642672
03-01-2018:01:38:46,852 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5650, total_loss/examples = 1.87514185654
03-01-2018:01:38:53,244 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5660, total_loss/examples = 1.87392698554
03-01-2018:01:38:59,583 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5670, total_loss/examples = 1.87310605544
03-01-2018:01:39:05,696 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5680, total_loss/examples = 1.87238493262
03-01-2018:01:39:11,777 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5690, total_loss/examples = 1.87088661752
03-01-2018:01:39:18,42 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5700, total_loss/examples = 1.86993216948
03-01-2018:01:39:24,284 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5710, total_loss/examples = 1.86887640537
03-01-2018:01:39:30,400 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5720, total_loss/examples = 1.86769445179
03-01-2018:01:39:36,557 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5730, total_loss/examples = 1.86673746227
03-01-2018:01:39:42,495 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5740, total_loss/examples = 1.86606638934
03-01-2018:01:39:48,648 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5750, total_loss/examples = 1.86493370588
03-01-2018:01:39:53,761 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5760, total_loss/examples = 1.86381651214
03-01-2018:01:39:59,774 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5770, total_loss/examples = 1.8625097581
03-01-2018:01:40:05,640 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5780, total_loss/examples = 1.86139923257
03-01-2018:01:40:11,745 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5790, total_loss/examples = 1.86036409419
03-01-2018:01:40:18,23 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5800, total_loss/examples = 1.85948499192
03-01-2018:01:40:24,242 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5810, total_loss/examples = 1.85836236454
03-01-2018:01:40:30,319 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5820, total_loss/examples = 1.85745764009
03-01-2018:01:40:36,719 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5830, total_loss/examples = 1.85643310345
03-01-2018:01:40:42,837 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5840, total_loss/examples = 1.85551425108
03-01-2018:01:40:48,985 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5850, total_loss/examples = 1.85459226258
03-01-2018:01:40:54,963 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5860, total_loss/examples = 1.85399906731
03-01-2018:01:41:00,100 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5870, total_loss/examples = 1.85298033906
03-01-2018:01:41:06,528 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5880, total_loss/examples = 1.85226587425
03-01-2018:01:41:12,885 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5890, total_loss/examples = 1.85140520374
03-01-2018:01:41:19,245 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5900, total_loss/examples = 1.85062365646
03-01-2018:01:41:25,222 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5910, total_loss/examples = 1.84963589925
03-01-2018:01:41:31,347 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5920, total_loss/examples = 1.84853345305
03-01-2018:01:41:37,769 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5930, total_loss/examples = 1.84726196082
03-01-2018:01:41:43,851 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5940, total_loss/examples = 1.84615707021
03-01-2018:01:41:50,91 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5950, total_loss/examples = 1.84500384497
03-01-2018:01:41:56,163 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5960, total_loss/examples = 1.84378584717
03-01-2018:01:42:02,754 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5970, total_loss/examples = 1.84291899769
03-01-2018:01:42:09,46 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5980, total_loss/examples = 1.84200837205
03-01-2018:01:42:15,61 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 5990, total_loss/examples = 1.84103433936
03-01-2018:01:42:21,590 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6000, total_loss/examples = 1.8398916139
03-01-2018:01:42:27,727 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6010, total_loss/examples = 1.8387528965
03-01-2018:01:42:32,962 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6020, total_loss/examples = 1.83802345836
03-01-2018:01:42:38,829 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6030, total_loss/examples = 1.83713596362
03-01-2018:01:42:44,997 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6040, total_loss/examples = 1.83620819209
03-01-2018:01:42:51,161 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6050, total_loss/examples = 1.83516978485
03-01-2018:01:42:57,388 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6060, total_loss/examples = 1.8344295491
03-01-2018:01:43:03,507 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6070, total_loss/examples = 1.83363548444
03-01-2018:01:43:09,734 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6080, total_loss/examples = 1.83267436005
03-01-2018:01:43:15,734 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6090, total_loss/examples = 1.83193265107
03-01-2018:01:43:21,977 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6100, total_loss/examples = 1.83092369859
03-01-2018:01:43:28,252 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6110, total_loss/examples = 1.8297549348
03-01-2018:01:43:34,583 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6120, total_loss/examples = 1.82894410873
03-01-2018:01:43:41,259 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6130, total_loss/examples = 1.82793429039
03-01-2018:01:43:47,628 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6140, total_loss/examples = 1.82655973234
03-01-2018:01:43:54,61 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6150, total_loss/examples = 1.82595400813
03-01-2018:01:44:01,3 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6160, total_loss/examples = 1.82557392343
03-01-2018:01:44:08,790 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6170, total_loss/examples = 1.82471309215
03-01-2018:01:44:14,912 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6180, total_loss/examples = 1.82345704343
03-01-2018:01:44:19,928 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6190, total_loss/examples = 1.82248294298
03-01-2018:01:44:26,132 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6200, total_loss/examples = 1.82157964126
03-01-2018:01:44:32,482 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6210, total_loss/examples = 1.82076165607
03-01-2018:01:44:38,420 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6220, total_loss/examples = 1.81993004806
03-01-2018:01:44:44,371 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6230, total_loss/examples = 1.81901241036
03-01-2018:01:44:50,475 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6240, total_loss/examples = 1.81806829523
03-01-2018:01:44:56,435 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6250, total_loss/examples = 1.81711443223
03-01-2018:01:45:02,734 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6260, total_loss/examples = 1.81631268543
03-01-2018:01:45:08,660 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6270, total_loss/examples = 1.81555137879
03-01-2018:01:45:14,524 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6280, total_loss/examples = 1.81434687428
03-01-2018:01:45:20,964 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6290, total_loss/examples = 1.81343104991
03-01-2018:01:45:28,436 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6300, total_loss/examples = 1.81260990501
03-01-2018:01:45:35,68 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6310, total_loss/examples = 1.8116151192
03-01-2018:01:45:40,126 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6320, total_loss/examples = 1.81047980627
03-01-2018:01:45:46,178 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6330, total_loss/examples = 1.80930297207
03-01-2018:01:45:52,910 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6340, total_loss/examples = 1.80828506416
03-01-2018:01:45:58,993 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6350, total_loss/examples = 1.80726913002
03-01-2018:01:46:04,817 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6360, total_loss/examples = 1.80626551534
03-01-2018:01:46:10,951 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6370, total_loss/examples = 1.80538544496
03-01-2018:01:46:17,53 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6380, total_loss/examples = 1.80441099067
03-01-2018:01:46:23,520 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6390, total_loss/examples = 1.80342939746
03-01-2018:01:46:29,432 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6400, total_loss/examples = 1.80255714668
03-01-2018:01:46:35,369 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6410, total_loss/examples = 1.80191882086
03-01-2018:01:46:40,301 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6420, total_loss/examples = 1.80082552989
03-01-2018:01:46:46,704 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6430, total_loss/examples = 1.80005101762
03-01-2018:01:46:52,724 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6440, total_loss/examples = 1.79938087726
03-01-2018:01:46:58,744 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6450, total_loss/examples = 1.7983307801
03-01-2018:01:47:04,882 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6460, total_loss/examples = 1.79716060377
03-01-2018:01:47:11,86 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6470, total_loss/examples = 1.79646944892
03-01-2018:01:47:17,45 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6480, total_loss/examples = 1.79570189237
03-01-2018:01:47:23,257 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6490, total_loss/examples = 1.79488506901
03-01-2018:01:47:29,409 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6500, total_loss/examples = 1.79439110884
03-01-2018:01:47:35,890 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6510, total_loss/examples = 1.79360010301
03-01-2018:01:47:41,862 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6520, total_loss/examples = 1.79275367306
03-01-2018:01:47:47,889 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6530, total_loss/examples = 1.79167335424
03-01-2018:01:47:53,130 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6540, total_loss/examples = 1.79086843098
03-01-2018:01:47:58,988 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6550, total_loss/examples = 1.78999652324
03-01-2018:01:48:05,145 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6560, total_loss/examples = 1.78911378811
03-01-2018:01:48:11,565 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6570, total_loss/examples = 1.78840001675
03-01-2018:01:48:18,70 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6580, total_loss/examples = 1.78771661306
03-01-2018:01:48:24,95 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6590, total_loss/examples = 1.78707847379
03-01-2018:01:48:30,235 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6600, total_loss/examples = 1.78618925825
03-01-2018:01:48:36,259 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6610, total_loss/examples = 1.78536877809
03-01-2018:01:48:42,247 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6620, total_loss/examples = 1.78442032773
03-01-2018:01:48:48,300 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6630, total_loss/examples = 1.78362981622
03-01-2018:01:48:54,234 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6640, total_loss/examples = 1.78311693382
03-01-2018:01:49:00,164 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6650, total_loss/examples = 1.78200856126
03-01-2018:01:49:06,301 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6660, total_loss/examples = 1.7811561472
03-01-2018:01:49:12,513 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6670, total_loss/examples = 1.7802142271
03-01-2018:01:49:17,663 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6680, total_loss/examples = 1.7798562965
03-01-2018:01:49:23,529 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6690, total_loss/examples = 1.77941058669
03-01-2018:01:49:29,785 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6700, total_loss/examples = 1.7787480152
03-01-2018:01:49:35,782 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6710, total_loss/examples = 1.77806581747
03-01-2018:01:49:42,418 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6720, total_loss/examples = 1.77724536548
03-01-2018:01:49:48,387 INFO     [ASReaderTrainer.py:141] Epoch 1/5, minibatch = 6730, total_loss/examples = 1.77640869011
03-01-2018:01:49:54,7 INFO     [ASReaderTrainer.py:161] Epoch 1/5, total_loss/examples_seen = 1.77581205781
03-01-2018:01:49:54,415 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 0, total_loss/examples = 1.77569499434
03-01-2018:01:49:54,415 INFO     [ASReaderTrainer.py:191] Starting to calculate accuracy
03-01-2018:01:49:54,416 INFO     [ASReaderTrainer.py:185] Starting to predict
03-01-2018:01:52:18,530 INFO     [ASReaderTrainer.py:146] valid_accuracy = 0.593671521848
03-01-2018:01:52:18,530 INFO     [ASReaderTrainer.py:149] previous valid accuracy = 0.553490708187, current valid accuracy = 0.593671521848
03-01-2018:01:52:18,542 INFO     [ASReaderTrainer.py:215] Saving model in file: /home/shantanu/PycharmProjects/attentionSum/generated_data/model_save.txt
03-01-2018:01:52:21,415 INFO     [ASReaderTrainer.py:218] Saving model args in file: /home/shantanu/PycharmProjects/attentionSum/generated_data/model_args_save.txt
03-01-2018:01:52:21,416 INFO     [ASReaderTrainer.py:221] Done saving model and model args
03-01-2018:01:52:28,191 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 10, total_loss/examples = 1.77453051526
03-01-2018:01:52:34,378 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 20, total_loss/examples = 1.77351770247
03-01-2018:01:52:40,345 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 30, total_loss/examples = 1.77233529276
03-01-2018:01:52:46,550 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 40, total_loss/examples = 1.77129021731
03-01-2018:01:52:52,429 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 50, total_loss/examples = 1.77039362622
03-01-2018:01:52:57,437 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 60, total_loss/examples = 1.76960298432
03-01-2018:01:53:03,564 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 70, total_loss/examples = 1.76854779315
03-01-2018:01:53:09,711 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 80, total_loss/examples = 1.76744782616
03-01-2018:01:53:15,769 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 90, total_loss/examples = 1.7664447717
03-01-2018:01:53:21,892 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 100, total_loss/examples = 1.76558500607
03-01-2018:01:53:28,8 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 110, total_loss/examples = 1.76455406485
03-01-2018:01:53:33,930 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 120, total_loss/examples = 1.76330297135
03-01-2018:01:53:39,858 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 130, total_loss/examples = 1.76247128335
03-01-2018:01:53:45,877 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 140, total_loss/examples = 1.7614248621
03-01-2018:01:53:51,709 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 150, total_loss/examples = 1.76018719948
03-01-2018:01:53:57,735 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 160, total_loss/examples = 1.75929260616
03-01-2018:01:54:04,332 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 170, total_loss/examples = 1.75851107034
03-01-2018:01:54:10,489 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 180, total_loss/examples = 1.75727822907
03-01-2018:01:54:15,712 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 190, total_loss/examples = 1.75616113201
03-01-2018:01:54:21,915 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 200, total_loss/examples = 1.75519386201
03-01-2018:01:54:28,0 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 210, total_loss/examples = 1.75418663026
03-01-2018:01:54:33,899 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 220, total_loss/examples = 1.75323708748
03-01-2018:01:54:40,15 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 230, total_loss/examples = 1.75223350732
03-01-2018:01:54:45,844 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 240, total_loss/examples = 1.75104072852
03-01-2018:01:54:52,229 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 250, total_loss/examples = 1.75018357418
03-01-2018:01:54:58,149 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 260, total_loss/examples = 1.74919977709
03-01-2018:01:55:04,220 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 270, total_loss/examples = 1.74792148607
03-01-2018:01:55:10,482 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 280, total_loss/examples = 1.74710466729
03-01-2018:01:55:16,562 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 290, total_loss/examples = 1.74635587733
03-01-2018:01:55:22,425 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 300, total_loss/examples = 1.7452571066
03-01-2018:01:55:27,847 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 310, total_loss/examples = 1.74407397098
03-01-2018:01:55:34,371 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 320, total_loss/examples = 1.74305290609
03-01-2018:01:55:40,575 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 330, total_loss/examples = 1.74219078651
03-01-2018:01:55:46,733 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 340, total_loss/examples = 1.7412689965
03-01-2018:01:55:52,975 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 350, total_loss/examples = 1.73996330477
03-01-2018:01:55:58,909 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 360, total_loss/examples = 1.7386712017
03-01-2018:01:56:05,24 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 370, total_loss/examples = 1.73774308324
03-01-2018:01:56:11,440 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 380, total_loss/examples = 1.73692432888
03-01-2018:01:56:17,392 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 390, total_loss/examples = 1.73584603069
03-01-2018:01:56:23,229 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 400, total_loss/examples = 1.7347146003
03-01-2018:01:56:29,139 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 410, total_loss/examples = 1.73402539213
03-01-2018:01:56:35,143 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 420, total_loss/examples = 1.73287513342
03-01-2018:01:56:42,84 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 430, total_loss/examples = 1.73190725997
03-01-2018:01:56:48,105 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 440, total_loss/examples = 1.73121519322
03-01-2018:01:56:54,145 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 450, total_loss/examples = 1.73044304617
03-01-2018:01:57:00,357 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 460, total_loss/examples = 1.72967484604
03-01-2018:01:57:05,641 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 470, total_loss/examples = 1.72871684842
03-01-2018:01:57:11,546 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 480, total_loss/examples = 1.72753861729
03-01-2018:01:57:17,619 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 490, total_loss/examples = 1.72675075122
03-01-2018:01:57:23,667 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 500, total_loss/examples = 1.72586951005
03-01-2018:01:57:29,632 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 510, total_loss/examples = 1.72499427606
03-01-2018:01:57:35,749 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 520, total_loss/examples = 1.72407650289
03-01-2018:01:57:41,811 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 530, total_loss/examples = 1.72310583275
03-01-2018:01:57:47,706 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 540, total_loss/examples = 1.72204446156
03-01-2018:01:57:53,796 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 550, total_loss/examples = 1.72108831675
03-01-2018:01:58:00,162 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 560, total_loss/examples = 1.71986309132
03-01-2018:01:58:06,187 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 570, total_loss/examples = 1.71900294756
03-01-2018:01:58:12,220 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 580, total_loss/examples = 1.71816813199
03-01-2018:01:58:17,480 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 590, total_loss/examples = 1.71707712892
03-01-2018:01:58:24,60 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 600, total_loss/examples = 1.71616911483
03-01-2018:01:58:30,202 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 610, total_loss/examples = 1.71530950567
03-01-2018:01:58:36,34 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 620, total_loss/examples = 1.71435324385
03-01-2018:01:58:42,241 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 630, total_loss/examples = 1.7133229519
03-01-2018:01:58:48,314 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 640, total_loss/examples = 1.71226765683
03-01-2018:01:58:54,375 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 650, total_loss/examples = 1.7114584385
03-01-2018:01:59:00,465 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 660, total_loss/examples = 1.71068493961
03-01-2018:01:59:06,525 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 670, total_loss/examples = 1.7095443699
03-01-2018:01:59:12,190 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 680, total_loss/examples = 1.70847525016
03-01-2018:01:59:18,76 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 690, total_loss/examples = 1.70750546003
03-01-2018:01:59:23,193 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 700, total_loss/examples = 1.70653477962
03-01-2018:01:59:29,321 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 710, total_loss/examples = 1.70555227166
03-01-2018:01:59:35,555 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 720, total_loss/examples = 1.70494080832
03-01-2018:01:59:41,784 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 730, total_loss/examples = 1.70415563782
03-01-2018:01:59:49,684 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 740, total_loss/examples = 1.7031769036
03-01-2018:01:59:56,205 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 750, total_loss/examples = 1.70217732872
03-01-2018:02:00:02,300 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 760, total_loss/examples = 1.70139266031
03-01-2018:02:00:08,484 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 770, total_loss/examples = 1.70046843337
03-01-2018:02:00:14,426 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 780, total_loss/examples = 1.69951254709
03-01-2018:02:00:20,386 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 790, total_loss/examples = 1.69860678635
03-01-2018:02:00:26,500 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 800, total_loss/examples = 1.6978089187
03-01-2018:02:00:32,558 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 810, total_loss/examples = 1.69685898117
03-01-2018:02:00:38,786 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 820, total_loss/examples = 1.69606498001
03-01-2018:02:00:44,773 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 830, total_loss/examples = 1.6952058177
03-01-2018:02:00:49,954 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 840, total_loss/examples = 1.69412167901
03-01-2018:02:00:55,932 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 850, total_loss/examples = 1.69348796858
03-01-2018:02:01:01,974 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 860, total_loss/examples = 1.69239019698
03-01-2018:02:01:08,496 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 870, total_loss/examples = 1.69149907778
03-01-2018:02:01:14,541 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 880, total_loss/examples = 1.69049713345
03-01-2018:02:01:20,710 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 890, total_loss/examples = 1.68966853091
03-01-2018:02:01:26,773 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 900, total_loss/examples = 1.68885138667
03-01-2018:02:01:32,674 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 910, total_loss/examples = 1.68800153835
03-01-2018:02:01:38,696 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 920, total_loss/examples = 1.68733860404
03-01-2018:02:01:44,678 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 930, total_loss/examples = 1.68659404315
03-01-2018:02:01:50,596 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 940, total_loss/examples = 1.68552614229
03-01-2018:02:01:56,579 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 950, total_loss/examples = 1.68487049967
03-01-2018:02:02:02,696 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 960, total_loss/examples = 1.68432158942
03-01-2018:02:02:07,715 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 970, total_loss/examples = 1.68337602469
03-01-2018:02:02:13,647 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 980, total_loss/examples = 1.68239782536
03-01-2018:02:02:19,773 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 990, total_loss/examples = 1.6815688506
03-01-2018:02:02:25,729 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1000, total_loss/examples = 1.68084646333
03-01-2018:02:02:31,674 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1010, total_loss/examples = 1.68009672757
03-01-2018:02:02:37,929 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1020, total_loss/examples = 1.67914857274
03-01-2018:02:02:43,977 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1030, total_loss/examples = 1.67848160105
03-01-2018:02:02:49,861 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1040, total_loss/examples = 1.67789428733
03-01-2018:02:02:55,759 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1050, total_loss/examples = 1.67703854178
03-01-2018:02:03:02,236 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1060, total_loss/examples = 1.67648299192
03-01-2018:02:03:07,889 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1070, total_loss/examples = 1.67564290641
03-01-2018:02:03:14,289 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1080, total_loss/examples = 1.67495361598
03-01-2018:02:03:20,618 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1090, total_loss/examples = 1.67405118099
03-01-2018:02:03:27,141 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1100, total_loss/examples = 1.67340653817
03-01-2018:02:03:33,419 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1110, total_loss/examples = 1.67237433723
03-01-2018:02:03:39,546 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1120, total_loss/examples = 1.67171963502
03-01-2018:02:03:45,634 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1130, total_loss/examples = 1.67096331403
03-01-2018:02:03:51,737 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1140, total_loss/examples = 1.67023965098
03-01-2018:02:03:57,969 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1150, total_loss/examples = 1.66930349144
03-01-2018:02:04:04,23 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1160, total_loss/examples = 1.66870217164
03-01-2018:02:04:10,112 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1170, total_loss/examples = 1.66773073769
03-01-2018:02:04:16,312 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1180, total_loss/examples = 1.66684374396
03-01-2018:02:04:22,453 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1190, total_loss/examples = 1.66608595789
03-01-2018:02:04:27,587 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1200, total_loss/examples = 1.66533000067
03-01-2018:02:04:33,671 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1210, total_loss/examples = 1.66445189525
03-01-2018:02:04:39,659 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1220, total_loss/examples = 1.66358268083
03-01-2018:02:04:45,680 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1230, total_loss/examples = 1.66248612218
03-01-2018:02:04:51,684 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1240, total_loss/examples = 1.66173020466
03-01-2018:02:04:57,905 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1250, total_loss/examples = 1.66105394171
03-01-2018:02:05:03,985 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1260, total_loss/examples = 1.66013902062
03-01-2018:02:05:10,476 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1270, total_loss/examples = 1.65941180371
03-01-2018:02:05:16,582 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1280, total_loss/examples = 1.65876898931
03-01-2018:02:05:21,745 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1290, total_loss/examples = 1.65791638102
03-01-2018:02:05:27,714 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1300, total_loss/examples = 1.65701585284
03-01-2018:02:05:33,729 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1310, total_loss/examples = 1.65625445843
03-01-2018:02:05:39,868 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1320, total_loss/examples = 1.65551546114
03-01-2018:02:05:46,140 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1330, total_loss/examples = 1.6545886842
03-01-2018:02:05:52,267 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1340, total_loss/examples = 1.6539209308
03-01-2018:02:05:58,350 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1350, total_loss/examples = 1.65312492814
03-01-2018:02:06:04,152 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1360, total_loss/examples = 1.65231583346
03-01-2018:02:06:10,332 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1370, total_loss/examples = 1.65137474629
03-01-2018:02:06:15,558 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1380, total_loss/examples = 1.65056229466
03-01-2018:02:06:21,520 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1390, total_loss/examples = 1.64980023968
03-01-2018:02:06:27,506 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1400, total_loss/examples = 1.64893034674
03-01-2018:02:06:33,604 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1410, total_loss/examples = 1.64807543044
03-01-2018:02:06:39,776 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1420, total_loss/examples = 1.64732539792
03-01-2018:02:06:46,263 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1430, total_loss/examples = 1.64649292446
03-01-2018:02:06:52,192 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1440, total_loss/examples = 1.64590861527
03-01-2018:02:06:58,324 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1450, total_loss/examples = 1.64502066915
03-01-2018:02:07:04,381 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1460, total_loss/examples = 1.64415665198
03-01-2018:02:07:09,933 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1470, total_loss/examples = 1.64322228608
03-01-2018:02:07:15,865 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1480, total_loss/examples = 1.64230882912
03-01-2018:02:07:22,67 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1490, total_loss/examples = 1.64145630713
03-01-2018:02:07:28,140 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1500, total_loss/examples = 1.64061882275
03-01-2018:02:07:35,369 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1510, total_loss/examples = 1.63979024504
03-01-2018:02:07:41,475 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1520, total_loss/examples = 1.63914403983
03-01-2018:02:07:47,367 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1530, total_loss/examples = 1.63842816601
03-01-2018:02:07:53,772 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1540, total_loss/examples = 1.63760258765
03-01-2018:02:07:59,656 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1550, total_loss/examples = 1.63679064358
03-01-2018:02:08:04,935 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1560, total_loss/examples = 1.63609540259
03-01-2018:02:08:11,292 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1570, total_loss/examples = 1.63553284669
03-01-2018:02:08:17,485 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1580, total_loss/examples = 1.63469892106
03-01-2018:02:08:24,825 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1590, total_loss/examples = 1.63378805045
03-01-2018:02:08:32,151 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1600, total_loss/examples = 1.63316099982
03-01-2018:02:08:38,187 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1610, total_loss/examples = 1.63265427756
03-01-2018:02:08:44,680 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1620, total_loss/examples = 1.631860048
03-01-2018:02:08:50,800 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1630, total_loss/examples = 1.63104573993
03-01-2018:02:08:56,949 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1640, total_loss/examples = 1.63043492316
03-01-2018:02:09:03,35 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1650, total_loss/examples = 1.62962666567
03-01-2018:02:09:09,117 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1660, total_loss/examples = 1.62926701288
03-01-2018:02:09:15,76 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1670, total_loss/examples = 1.62855713938
03-01-2018:02:09:20,251 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1680, total_loss/examples = 1.62774586793
03-01-2018:02:09:26,98 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1690, total_loss/examples = 1.62690488154
03-01-2018:02:09:31,988 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1700, total_loss/examples = 1.62606867408
03-01-2018:02:09:37,978 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1710, total_loss/examples = 1.62533580189
03-01-2018:02:09:43,865 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1720, total_loss/examples = 1.62455861556
03-01-2018:02:09:49,799 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1730, total_loss/examples = 1.6236543951
03-01-2018:02:09:55,797 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1740, total_loss/examples = 1.62282134323
03-01-2018:02:10:01,807 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1750, total_loss/examples = 1.62223871071
03-01-2018:02:10:07,998 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1760, total_loss/examples = 1.62180344854
03-01-2018:02:10:14,34 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1770, total_loss/examples = 1.62100392703
03-01-2018:02:10:20,72 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1780, total_loss/examples = 1.62038113951
03-01-2018:02:10:25,231 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1790, total_loss/examples = 1.61948344054
03-01-2018:02:10:31,432 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1800, total_loss/examples = 1.61874340996
03-01-2018:02:10:37,447 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1810, total_loss/examples = 1.61810147229
03-01-2018:02:10:43,394 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1820, total_loss/examples = 1.61745355723
03-01-2018:02:10:49,546 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1830, total_loss/examples = 1.61678963987
03-01-2018:02:10:55,455 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1840, total_loss/examples = 1.61592050925
03-01-2018:02:11:01,441 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1850, total_loss/examples = 1.61495041323
03-01-2018:02:11:07,354 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1860, total_loss/examples = 1.61414327517
03-01-2018:02:11:13,352 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1870, total_loss/examples = 1.61334085512
03-01-2018:02:11:18,524 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1880, total_loss/examples = 1.61274038903
03-01-2018:02:11:24,516 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1890, total_loss/examples = 1.61178988137
03-01-2018:02:11:30,628 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1900, total_loss/examples = 1.61122440589
03-01-2018:02:11:36,551 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1910, total_loss/examples = 1.61049256356
03-01-2018:02:11:42,770 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1920, total_loss/examples = 1.60976874623
03-01-2018:02:11:48,821 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1930, total_loss/examples = 1.60924136264
03-01-2018:02:11:54,854 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1940, total_loss/examples = 1.60851255859
03-01-2018:02:12:01,142 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1950, total_loss/examples = 1.60811903636
03-01-2018:02:12:07,102 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1960, total_loss/examples = 1.60737092517
03-01-2018:02:12:13,356 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1970, total_loss/examples = 1.60659659754
03-01-2018:02:12:19,794 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1980, total_loss/examples = 1.60581195531
03-01-2018:02:12:25,842 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 1990, total_loss/examples = 1.60517821178
03-01-2018:02:12:30,969 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2000, total_loss/examples = 1.60449629221
03-01-2018:02:12:36,931 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2010, total_loss/examples = 1.60397473783
03-01-2018:02:12:42,713 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2020, total_loss/examples = 1.60328437331
03-01-2018:02:12:48,509 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2030, total_loss/examples = 1.60249613587
03-01-2018:02:12:54,334 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2040, total_loss/examples = 1.60173223008
03-01-2018:02:13:00,58 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2050, total_loss/examples = 1.60112786621
03-01-2018:02:13:05,902 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2060, total_loss/examples = 1.60044658003
03-01-2018:02:13:11,785 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2070, total_loss/examples = 1.59995137693
03-01-2018:02:13:16,949 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2080, total_loss/examples = 1.59940765559
03-01-2018:02:13:23,74 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2090, total_loss/examples = 1.59876469957
03-01-2018:02:13:28,892 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2100, total_loss/examples = 1.59821816886
03-01-2018:02:13:34,974 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2110, total_loss/examples = 1.59761557608
03-01-2018:02:13:41,390 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2120, total_loss/examples = 1.59688140881
03-01-2018:02:13:47,582 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2130, total_loss/examples = 1.5962084345
03-01-2018:02:13:53,438 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2140, total_loss/examples = 1.59574383084
03-01-2018:02:13:59,526 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2150, total_loss/examples = 1.59506604478
03-01-2018:02:14:05,642 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2160, total_loss/examples = 1.59451527052
03-01-2018:02:14:11,920 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2170, total_loss/examples = 1.5939634296
03-01-2018:02:14:17,944 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2180, total_loss/examples = 1.59356371904
03-01-2018:02:14:24,59 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2190, total_loss/examples = 1.59271879856
03-01-2018:02:14:29,918 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2200, total_loss/examples = 1.59229802498
03-01-2018:02:14:36,237 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2210, total_loss/examples = 1.59179747071
03-01-2018:02:14:42,198 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2220, total_loss/examples = 1.591120894
03-01-2018:02:14:49,29 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2230, total_loss/examples = 1.59050455883
03-01-2018:02:14:55,63 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2240, total_loss/examples = 1.58980175678
03-01-2018:02:15:00,132 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2250, total_loss/examples = 1.58886083841
03-01-2018:02:15:06,706 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2260, total_loss/examples = 1.58800811483
03-01-2018:02:15:12,925 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2270, total_loss/examples = 1.58754344526
03-01-2018:02:15:19,10 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2280, total_loss/examples = 1.58699768118
03-01-2018:02:15:25,300 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2290, total_loss/examples = 1.58639367723
03-01-2018:02:15:31,476 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2300, total_loss/examples = 1.585610377
03-01-2018:02:15:37,535 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2310, total_loss/examples = 1.58483655155
03-01-2018:02:15:43,715 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2320, total_loss/examples = 1.58420946812
03-01-2018:02:15:49,750 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2330, total_loss/examples = 1.58344205631
03-01-2018:02:15:56,55 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2340, total_loss/examples = 1.58276762628
03-01-2018:02:16:02,320 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2350, total_loss/examples = 1.58228513756
03-01-2018:02:16:08,263 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2360, total_loss/examples = 1.58147761126
03-01-2018:02:16:14,344 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2370, total_loss/examples = 1.58086109503
03-01-2018:02:16:20,416 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2380, total_loss/examples = 1.58022975001
03-01-2018:02:16:26,409 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2390, total_loss/examples = 1.57937484252
03-01-2018:02:16:32,316 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2400, total_loss/examples = 1.5786431341
03-01-2018:02:16:38,279 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2410, total_loss/examples = 1.5781727155
03-01-2018:02:16:44,477 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2420, total_loss/examples = 1.57767359542
03-01-2018:02:16:49,631 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2430, total_loss/examples = 1.57708046405
03-01-2018:02:16:55,663 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2440, total_loss/examples = 1.57661708399
03-01-2018:02:17:01,488 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2450, total_loss/examples = 1.57605601195
03-01-2018:02:17:07,338 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2460, total_loss/examples = 1.57532105837
03-01-2018:02:17:13,418 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2470, total_loss/examples = 1.57477035144
03-01-2018:02:17:19,578 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2480, total_loss/examples = 1.57421501845
03-01-2018:02:17:25,607 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2490, total_loss/examples = 1.57367281003
03-01-2018:02:17:31,789 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2500, total_loss/examples = 1.57304815277
03-01-2018:02:17:37,842 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2510, total_loss/examples = 1.57239218566
03-01-2018:02:17:43,878 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2520, total_loss/examples = 1.57190308006
03-01-2018:02:17:50,440 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2530, total_loss/examples = 1.57123654953
03-01-2018:02:17:55,668 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2540, total_loss/examples = 1.57053571828
03-01-2018:02:18:02,337 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2550, total_loss/examples = 1.56995107407
03-01-2018:02:18:08,874 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2560, total_loss/examples = 1.56923348291
03-01-2018:02:18:15,176 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2570, total_loss/examples = 1.56850245994
03-01-2018:02:18:21,930 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2580, total_loss/examples = 1.56795763586
03-01-2018:02:18:28,200 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2590, total_loss/examples = 1.56740425007
03-01-2018:02:18:34,441 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2600, total_loss/examples = 1.56684405448
03-01-2018:02:18:40,532 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2610, total_loss/examples = 1.56623314384
03-01-2018:02:18:46,579 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2620, total_loss/examples = 1.56574843852
03-01-2018:02:18:52,612 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2630, total_loss/examples = 1.56506247245
03-01-2018:02:18:58,819 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2640, total_loss/examples = 1.56428248207
03-01-2018:02:19:04,818 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2650, total_loss/examples = 1.56381222431
03-01-2018:02:19:11,275 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2660, total_loss/examples = 1.5632958005
03-01-2018:02:19:17,133 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2670, total_loss/examples = 1.56266333211
03-01-2018:02:19:23,224 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2680, total_loss/examples = 1.56206594851
03-01-2018:02:19:29,466 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2690, total_loss/examples = 1.5614905027
03-01-2018:02:19:35,647 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2700, total_loss/examples = 1.5611126929
03-01-2018:02:19:40,882 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2710, total_loss/examples = 1.56030307511
03-01-2018:02:19:46,698 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2720, total_loss/examples = 1.55963502489
03-01-2018:02:19:52,667 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2730, total_loss/examples = 1.55886827646
03-01-2018:02:19:58,890 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2740, total_loss/examples = 1.55828765396
03-01-2018:02:20:05,236 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2750, total_loss/examples = 1.55767128083
03-01-2018:02:20:11,174 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2760, total_loss/examples = 1.55708620079
03-01-2018:02:20:17,529 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2770, total_loss/examples = 1.55646970003
03-01-2018:02:20:23,651 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2780, total_loss/examples = 1.55592846534
03-01-2018:02:20:28,742 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2790, total_loss/examples = 1.55546189965
03-01-2018:02:20:34,849 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2800, total_loss/examples = 1.55496069119
03-01-2018:02:20:41,191 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2810, total_loss/examples = 1.5542002364
03-01-2018:02:20:47,280 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2820, total_loss/examples = 1.55338410006
03-01-2018:02:20:53,535 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2830, total_loss/examples = 1.55282879949
03-01-2018:02:20:59,664 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2840, total_loss/examples = 1.5523050244
03-01-2018:02:21:05,620 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2850, total_loss/examples = 1.5515242349
03-01-2018:02:21:11,859 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2860, total_loss/examples = 1.55107569615
03-01-2018:02:21:18,138 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2870, total_loss/examples = 1.55041440525
03-01-2018:02:21:23,275 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2880, total_loss/examples = 1.54983593346
03-01-2018:02:21:29,486 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2890, total_loss/examples = 1.54930234048
03-01-2018:02:21:35,761 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2900, total_loss/examples = 1.54888678913
03-01-2018:02:21:43,253 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2910, total_loss/examples = 1.54836665443
03-01-2018:02:21:49,534 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2920, total_loss/examples = 1.54786719755
03-01-2018:02:21:55,716 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2930, total_loss/examples = 1.54723659652
03-01-2018:02:22:01,794 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2940, total_loss/examples = 1.54652030045
03-01-2018:02:22:08,67 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2950, total_loss/examples = 1.54589800255
03-01-2018:02:22:13,882 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2960, total_loss/examples = 1.54516503336
03-01-2018:02:22:20,91 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2970, total_loss/examples = 1.54450321495
03-01-2018:02:22:26,813 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2980, total_loss/examples = 1.54398746112
03-01-2018:02:22:33,70 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 2990, total_loss/examples = 1.54339603156
03-01-2018:02:22:39,91 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3000, total_loss/examples = 1.54286715111
03-01-2018:02:22:45,241 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3010, total_loss/examples = 1.54215369216
03-01-2018:02:22:51,343 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3020, total_loss/examples = 1.541603392
03-01-2018:02:22:57,493 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3030, total_loss/examples = 1.54110784321
03-01-2018:02:23:02,568 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3040, total_loss/examples = 1.54057353874
03-01-2018:02:23:08,689 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3050, total_loss/examples = 1.54000401343
03-01-2018:02:23:14,868 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3060, total_loss/examples = 1.53952387324
03-01-2018:02:23:20,994 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3070, total_loss/examples = 1.53909614206
03-01-2018:02:23:27,70 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3080, total_loss/examples = 1.53854437423
03-01-2018:02:23:34,538 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3090, total_loss/examples = 1.53803217568
03-01-2018:02:23:41,791 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3100, total_loss/examples = 1.53747434591
03-01-2018:02:23:47,719 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3110, total_loss/examples = 1.53705193806
03-01-2018:02:23:52,769 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3120, total_loss/examples = 1.5366913106
03-01-2018:02:23:59,339 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3130, total_loss/examples = 1.53614945473
03-01-2018:02:24:06,13 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3140, total_loss/examples = 1.53580361745
03-01-2018:02:24:12,79 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3150, total_loss/examples = 1.53524986634
03-01-2018:02:24:18,16 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3160, total_loss/examples = 1.53478191596
03-01-2018:02:24:24,62 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3170, total_loss/examples = 1.53401975105
03-01-2018:02:24:30,13 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3180, total_loss/examples = 1.5333030599
03-01-2018:02:24:35,940 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3190, total_loss/examples = 1.53274751016
03-01-2018:02:24:41,894 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3200, total_loss/examples = 1.53213113718
03-01-2018:02:24:48,10 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3210, total_loss/examples = 1.53152880411
03-01-2018:02:24:54,49 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3220, total_loss/examples = 1.53090663052
03-01-2018:02:25:00,171 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3230, total_loss/examples = 1.53040220253
03-01-2018:02:25:06,469 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3240, total_loss/examples = 1.52981255649
03-01-2018:02:25:12,529 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3250, total_loss/examples = 1.52923184871
03-01-2018:02:25:19,45 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3260, total_loss/examples = 1.52865421051
03-01-2018:02:25:24,220 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3270, total_loss/examples = 1.52803719151
03-01-2018:02:25:30,610 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3280, total_loss/examples = 1.52740370331
03-01-2018:02:25:36,617 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3290, total_loss/examples = 1.52692911634
03-01-2018:02:25:43,12 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3300, total_loss/examples = 1.5265378884
03-01-2018:02:25:49,264 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3310, total_loss/examples = 1.52599905991
03-01-2018:02:25:55,153 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3320, total_loss/examples = 1.52545840393
03-01-2018:02:26:01,222 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3330, total_loss/examples = 1.52482644629
03-01-2018:02:26:07,286 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3340, total_loss/examples = 1.52433840235
03-01-2018:02:26:13,588 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3350, total_loss/examples = 1.5236457626
03-01-2018:02:26:19,618 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3360, total_loss/examples = 1.52317357533
03-01-2018:02:26:25,685 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3370, total_loss/examples = 1.52256614053
03-01-2018:02:26:26,881 INFO     [ASReaderTrainer.py:191] Starting to calculate accuracy
03-01-2018:02:26:26,881 INFO     [ASReaderTrainer.py:185] Starting to predict
03-01-2018:02:28:50,442 INFO     [ASReaderTrainer.py:146] valid_accuracy = 0.619286790558
03-01-2018:02:28:50,442 INFO     [ASReaderTrainer.py:149] previous valid accuracy = 0.593671521848, current valid accuracy = 0.619286790558
03-01-2018:02:28:50,455 INFO     [ASReaderTrainer.py:215] Saving model in file: /home/shantanu/PycharmProjects/attentionSum/generated_data/model_save.txt
03-01-2018:02:28:53,322 INFO     [ASReaderTrainer.py:218] Saving model args in file: /home/shantanu/PycharmProjects/attentionSum/generated_data/model_args_save.txt
03-01-2018:02:28:53,365 INFO     [ASReaderTrainer.py:221] Done saving model and model args
03-01-2018:02:28:58,297 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3380, total_loss/examples = 1.52213810363
03-01-2018:02:29:03,533 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3390, total_loss/examples = 1.52144677643
03-01-2018:02:29:09,885 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3400, total_loss/examples = 1.52106418971
03-01-2018:02:29:16,6 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3410, total_loss/examples = 1.52063413562
03-01-2018:02:29:21,877 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3420, total_loss/examples = 1.52004363029
03-01-2018:02:29:27,867 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3430, total_loss/examples = 1.51960478859
03-01-2018:02:29:33,910 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3440, total_loss/examples = 1.51910507364
03-01-2018:02:29:40,15 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3450, total_loss/examples = 1.51865355973
03-01-2018:02:29:46,17 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3460, total_loss/examples = 1.51804360436
03-01-2018:02:29:52,137 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3470, total_loss/examples = 1.51751765777
03-01-2018:02:29:58,363 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3480, total_loss/examples = 1.51683779305
03-01-2018:02:30:04,728 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3490, total_loss/examples = 1.51634889118
03-01-2018:02:30:11,294 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3500, total_loss/examples = 1.51583291351
03-01-2018:02:30:17,950 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3510, total_loss/examples = 1.51547454016
03-01-2018:02:30:24,351 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3520, total_loss/examples = 1.51482666812
03-01-2018:02:30:30,517 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3530, total_loss/examples = 1.5145853811
03-01-2018:02:30:36,886 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3540, total_loss/examples = 1.51428940357
03-01-2018:02:30:42,799 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3550, total_loss/examples = 1.51381112533
03-01-2018:02:30:47,827 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3560, total_loss/examples = 1.51322030961
03-01-2018:02:30:54,147 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3570, total_loss/examples = 1.51276801145
03-01-2018:02:31:00,83 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3580, total_loss/examples = 1.5122125778
03-01-2018:02:31:06,85 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3590, total_loss/examples = 1.51170567792
03-01-2018:02:31:11,840 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3600, total_loss/examples = 1.51128960238
03-01-2018:02:31:17,818 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3610, total_loss/examples = 1.51078437569
03-01-2018:02:31:23,872 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3620, total_loss/examples = 1.51022421097
03-01-2018:02:31:29,701 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3630, total_loss/examples = 1.50966969104
03-01-2018:02:31:34,897 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3640, total_loss/examples = 1.50947997889
03-01-2018:02:31:40,898 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3650, total_loss/examples = 1.50893421609
03-01-2018:02:31:46,984 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3660, total_loss/examples = 1.5084573073
03-01-2018:02:31:53,530 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3670, total_loss/examples = 1.50798292256
03-01-2018:02:31:59,675 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3680, total_loss/examples = 1.50750162728
03-01-2018:02:32:05,512 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3690, total_loss/examples = 1.50694147577
03-01-2018:02:32:11,669 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3700, total_loss/examples = 1.50636854589
03-01-2018:02:32:17,897 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3710, total_loss/examples = 1.50592873874
03-01-2018:02:32:24,216 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3720, total_loss/examples = 1.50529018411
03-01-2018:02:32:30,318 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3730, total_loss/examples = 1.50480055503
03-01-2018:02:32:36,239 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3740, total_loss/examples = 1.50440298754
03-01-2018:02:32:42,206 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3750, total_loss/examples = 1.50392851172
03-01-2018:02:32:48,416 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3760, total_loss/examples = 1.50348113705
03-01-2018:02:32:54,765 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3770, total_loss/examples = 1.50288886347
03-01-2018:02:33:00,592 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3780, total_loss/examples = 1.50249360013
03-01-2018:02:33:06,481 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3790, total_loss/examples = 1.50181517358
03-01-2018:02:33:12,476 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3800, total_loss/examples = 1.50125097571
03-01-2018:02:33:18,548 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3810, total_loss/examples = 1.5007526897
03-01-2018:02:33:23,882 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3820, total_loss/examples = 1.50016701838
03-01-2018:02:33:30,28 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3830, total_loss/examples = 1.49968974369
03-01-2018:02:33:37,73 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3840, total_loss/examples = 1.49926478834
03-01-2018:02:33:43,133 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3850, total_loss/examples = 1.49881398128
03-01-2018:02:33:48,750 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3860, total_loss/examples = 1.49834809406
03-01-2018:02:33:54,708 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3870, total_loss/examples = 1.49775739467
03-01-2018:02:34:00,782 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3880, total_loss/examples = 1.49748487251
03-01-2018:02:34:07,75 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3890, total_loss/examples = 1.49704965651
03-01-2018:02:34:13,65 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3900, total_loss/examples = 1.49666516878
03-01-2018:02:34:19,46 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3910, total_loss/examples = 1.49638742613
03-01-2018:02:34:24,879 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3920, total_loss/examples = 1.49600020275
03-01-2018:02:34:30,917 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3930, total_loss/examples = 1.49560568965
03-01-2018:02:34:37,63 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3940, total_loss/examples = 1.4952615199
03-01-2018:02:34:43,149 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3950, total_loss/examples = 1.49455159803
03-01-2018:02:34:49,697 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3960, total_loss/examples = 1.49412692169
03-01-2018:02:34:55,574 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3970, total_loss/examples = 1.49376200915
03-01-2018:02:35:01,588 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3980, total_loss/examples = 1.49337602223
03-01-2018:02:35:06,837 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 3990, total_loss/examples = 1.49287618965
03-01-2018:02:35:12,852 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4000, total_loss/examples = 1.4924588128
03-01-2018:02:35:18,710 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4010, total_loss/examples = 1.49199410614
03-01-2018:02:35:24,802 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4020, total_loss/examples = 1.49149883469
03-01-2018:02:35:31,72 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4030, total_loss/examples = 1.49104676843
03-01-2018:02:35:37,321 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4040, total_loss/examples = 1.49050570036
03-01-2018:02:35:43,412 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4050, total_loss/examples = 1.49002910179
03-01-2018:02:35:49,524 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4060, total_loss/examples = 1.48964453497
03-01-2018:02:35:55,662 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4070, total_loss/examples = 1.48909675233
03-01-2018:02:36:01,841 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4080, total_loss/examples = 1.48862125855
03-01-2018:02:36:07,922 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4090, total_loss/examples = 1.48829341558
03-01-2018:02:36:13,744 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4100, total_loss/examples = 1.48786447417
03-01-2018:02:36:19,621 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4110, total_loss/examples = 1.48748022257
03-01-2018:02:36:25,706 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4120, total_loss/examples = 1.48702009573
03-01-2018:02:36:31,753 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4130, total_loss/examples = 1.48651204038
03-01-2018:02:36:36,863 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4140, total_loss/examples = 1.48605468778
03-01-2018:02:36:43,152 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4150, total_loss/examples = 1.48555488162
03-01-2018:02:36:49,175 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4160, total_loss/examples = 1.48509866717
03-01-2018:02:36:54,976 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4170, total_loss/examples = 1.48477401628
03-01-2018:02:37:01,5 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4180, total_loss/examples = 1.48433478936
03-01-2018:02:37:06,995 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4190, total_loss/examples = 1.48404477383
03-01-2018:02:37:12,894 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4200, total_loss/examples = 1.48345091925
03-01-2018:02:37:19,36 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4210, total_loss/examples = 1.48289742403
03-01-2018:02:37:24,884 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4220, total_loss/examples = 1.48245637241
03-01-2018:02:37:31,128 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4230, total_loss/examples = 1.48226457337
03-01-2018:02:37:37,237 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4240, total_loss/examples = 1.48199083575
03-01-2018:02:37:42,497 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4250, total_loss/examples = 1.48160984047
03-01-2018:02:37:48,594 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4260, total_loss/examples = 1.48113530462
03-01-2018:02:37:54,723 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4270, total_loss/examples = 1.48075577564
03-01-2018:02:38:00,914 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4280, total_loss/examples = 1.48027917818
03-01-2018:02:38:07,316 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4290, total_loss/examples = 1.47966961544
03-01-2018:02:38:13,697 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4300, total_loss/examples = 1.4791692572
03-01-2018:02:38:19,788 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4310, total_loss/examples = 1.47870695441
03-01-2018:02:38:25,927 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4320, total_loss/examples = 1.47820797114
03-01-2018:02:38:31,897 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4330, total_loss/examples = 1.47786550533
03-01-2018:02:38:37,971 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4340, total_loss/examples = 1.47747209731
03-01-2018:02:38:44,14 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4350, total_loss/examples = 1.47723158545
03-01-2018:02:38:49,221 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4360, total_loss/examples = 1.47678168003
03-01-2018:02:38:55,510 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4370, total_loss/examples = 1.47637381338
03-01-2018:02:39:01,915 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4380, total_loss/examples = 1.47592655225
03-01-2018:02:39:07,912 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4390, total_loss/examples = 1.47542553323
03-01-2018:02:39:13,857 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4400, total_loss/examples = 1.47499117751
03-01-2018:02:39:20,144 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4410, total_loss/examples = 1.47451036802
03-01-2018:02:39:26,358 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4420, total_loss/examples = 1.47420618103
03-01-2018:02:39:32,211 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4430, total_loss/examples = 1.47378222003
03-01-2018:02:39:38,195 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4440, total_loss/examples = 1.47339804586
03-01-2018:02:39:44,91 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4450, total_loss/examples = 1.47288453294
03-01-2018:02:39:50,156 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4460, total_loss/examples = 1.47252522068
03-01-2018:02:39:56,218 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4470, total_loss/examples = 1.47213620872
03-01-2018:02:40:01,251 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4480, total_loss/examples = 1.4716634155
03-01-2018:02:40:07,269 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4490, total_loss/examples = 1.47128974987
03-01-2018:02:40:13,365 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4500, total_loss/examples = 1.47092341931
03-01-2018:02:40:19,254 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4510, total_loss/examples = 1.4704304444
03-01-2018:02:40:25,462 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4520, total_loss/examples = 1.46987592232
03-01-2018:02:40:31,448 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4530, total_loss/examples = 1.46951827567
03-01-2018:02:40:37,503 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4540, total_loss/examples = 1.46916072586
03-01-2018:02:40:43,564 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4550, total_loss/examples = 1.46875669079
03-01-2018:02:40:48,572 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4560, total_loss/examples = 1.46831664183
03-01-2018:02:40:54,677 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4570, total_loss/examples = 1.46806026198
03-01-2018:02:41:01,733 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4580, total_loss/examples = 1.46753045058
03-01-2018:02:41:06,936 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4590, total_loss/examples = 1.46692011071
03-01-2018:02:41:12,971 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4600, total_loss/examples = 1.46644198087
03-01-2018:02:41:19,56 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4610, total_loss/examples = 1.46610486981
03-01-2018:02:41:25,52 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4620, total_loss/examples = 1.46566572326
03-01-2018:02:41:30,892 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4630, total_loss/examples = 1.46523548892
03-01-2018:02:41:36,862 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4640, total_loss/examples = 1.46483294297
03-01-2018:02:41:43,123 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4650, total_loss/examples = 1.46436488316
03-01-2018:02:41:49,217 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4660, total_loss/examples = 1.46382406286
03-01-2018:02:41:55,521 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4670, total_loss/examples = 1.46339614044
03-01-2018:02:42:01,818 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4680, total_loss/examples = 1.46302461574
03-01-2018:02:42:07,799 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4690, total_loss/examples = 1.46266216395
03-01-2018:02:42:13,8 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4700, total_loss/examples = 1.46235742014
03-01-2018:02:42:20,182 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4710, total_loss/examples = 1.46194577093
03-01-2018:02:42:25,377 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4720, total_loss/examples = 1.46150811104
03-01-2018:02:42:31,445 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4730, total_loss/examples = 1.4611057069
03-01-2018:02:42:38,429 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4740, total_loss/examples = 1.46070487151
03-01-2018:02:42:44,552 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4750, total_loss/examples = 1.4601984231
03-01-2018:02:42:50,575 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4760, total_loss/examples = 1.45962693058
03-01-2018:02:42:57,61 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4770, total_loss/examples = 1.45934293137
03-01-2018:02:43:02,830 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4780, total_loss/examples = 1.4589439485
03-01-2018:02:43:09,37 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4790, total_loss/examples = 1.45857064775
03-01-2018:02:43:15,170 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4800, total_loss/examples = 1.45813887464
03-01-2018:02:43:20,473 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4810, total_loss/examples = 1.45763933171
03-01-2018:02:43:26,666 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4820, total_loss/examples = 1.45740565297
03-01-2018:02:43:32,560 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4830, total_loss/examples = 1.45699967612
03-01-2018:02:43:38,532 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4840, total_loss/examples = 1.45652266223
03-01-2018:02:43:44,543 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4850, total_loss/examples = 1.45614163321
03-01-2018:02:43:51,74 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4860, total_loss/examples = 1.45592826336
03-01-2018:02:43:57,19 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4870, total_loss/examples = 1.45553873945
03-01-2018:02:44:02,968 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4880, total_loss/examples = 1.45499784762
03-01-2018:02:44:09,115 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4890, total_loss/examples = 1.45449964301
03-01-2018:02:44:15,63 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4900, total_loss/examples = 1.45417266095
03-01-2018:02:44:21,139 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4910, total_loss/examples = 1.45369715991
03-01-2018:02:44:26,984 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4920, total_loss/examples = 1.45331836266
03-01-2018:02:44:33,97 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4930, total_loss/examples = 1.45292498153
03-01-2018:02:44:38,198 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4940, total_loss/examples = 1.4525448509
03-01-2018:02:44:44,280 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4950, total_loss/examples = 1.45223728885
03-01-2018:02:44:50,278 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4960, total_loss/examples = 1.45172269477
03-01-2018:02:44:56,433 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4970, total_loss/examples = 1.45124490939
03-01-2018:02:45:02,547 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4980, total_loss/examples = 1.45087501284
03-01-2018:02:45:08,504 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 4990, total_loss/examples = 1.45049217528
03-01-2018:02:45:14,461 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5000, total_loss/examples = 1.45012284777
03-01-2018:02:45:20,783 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5010, total_loss/examples = 1.44971611185
03-01-2018:02:45:27,146 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5020, total_loss/examples = 1.44926562304
03-01-2018:02:45:33,112 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5030, total_loss/examples = 1.44885362287
03-01-2018:02:45:39,333 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5040, total_loss/examples = 1.44851375241
03-01-2018:02:45:44,501 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5050, total_loss/examples = 1.44794577193
03-01-2018:02:45:50,690 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5060, total_loss/examples = 1.44751617735
03-01-2018:02:45:56,758 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5070, total_loss/examples = 1.44716077571
03-01-2018:02:46:02,700 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5080, total_loss/examples = 1.44675126153
03-01-2018:02:46:08,629 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5090, total_loss/examples = 1.44646989851
03-01-2018:02:46:15,7 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5100, total_loss/examples = 1.4460617422
03-01-2018:02:46:21,21 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5110, total_loss/examples = 1.44559004104
03-01-2018:02:46:27,56 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5120, total_loss/examples = 1.44523750792
03-01-2018:02:46:33,115 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5130, total_loss/examples = 1.44498805491
03-01-2018:02:46:39,241 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5140, total_loss/examples = 1.44441689685
03-01-2018:02:46:45,66 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5150, total_loss/examples = 1.44409133118
03-01-2018:02:46:50,288 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5160, total_loss/examples = 1.44371641691
03-01-2018:02:46:57,361 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5170, total_loss/examples = 1.44334921208
03-01-2018:02:47:03,375 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5180, total_loss/examples = 1.44303386802
03-01-2018:02:47:08,375 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5190, total_loss/examples = 1.44266245745
03-01-2018:02:47:14,370 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5200, total_loss/examples = 1.44225406108
03-01-2018:02:47:20,456 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5210, total_loss/examples = 1.44184245653
03-01-2018:02:47:26,894 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5220, total_loss/examples = 1.44147773825
03-01-2018:02:47:32,611 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5230, total_loss/examples = 1.4410340296
03-01-2018:02:47:38,561 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5240, total_loss/examples = 1.4407621042
03-01-2018:02:47:44,629 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5250, total_loss/examples = 1.44032803253
03-01-2018:02:47:49,743 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5260, total_loss/examples = 1.43996769979
03-01-2018:02:47:55,828 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5270, total_loss/examples = 1.43971243923
03-01-2018:02:48:01,980 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5280, total_loss/examples = 1.43947095857
03-01-2018:02:48:08,100 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5290, total_loss/examples = 1.43900295742
03-01-2018:02:48:14,27 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5300, total_loss/examples = 1.4387595204
03-01-2018:02:48:19,941 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5310, total_loss/examples = 1.43838166709
03-01-2018:02:48:26,98 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5320, total_loss/examples = 1.43797668999
03-01-2018:02:48:32,258 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5330, total_loss/examples = 1.43767517606
03-01-2018:02:48:38,351 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5340, total_loss/examples = 1.43712561446
03-01-2018:02:48:44,368 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5350, total_loss/examples = 1.43665419188
03-01-2018:02:48:50,197 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5360, total_loss/examples = 1.43633999702
03-01-2018:02:48:56,152 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5370, total_loss/examples = 1.43596650761
03-01-2018:02:49:01,424 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5380, total_loss/examples = 1.4355702698
03-01-2018:02:49:07,529 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5390, total_loss/examples = 1.43509182756
03-01-2018:02:49:13,585 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5400, total_loss/examples = 1.43469974859
03-01-2018:02:49:19,804 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5410, total_loss/examples = 1.43426828139
03-01-2018:02:49:25,614 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5420, total_loss/examples = 1.43394988549
03-01-2018:02:49:31,611 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5430, total_loss/examples = 1.43348353282
03-01-2018:02:49:37,779 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5440, total_loss/examples = 1.43321820583
03-01-2018:02:49:43,966 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5450, total_loss/examples = 1.43279118802
03-01-2018:02:49:50,89 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5460, total_loss/examples = 1.43241469884
03-01-2018:02:49:56,112 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5470, total_loss/examples = 1.43206074874
03-01-2018:02:50:01,884 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5480, total_loss/examples = 1.43153730312
03-01-2018:02:50:07,893 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5490, total_loss/examples = 1.43129651797
03-01-2018:02:50:12,974 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5500, total_loss/examples = 1.43114608455
03-01-2018:02:50:19,136 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5510, total_loss/examples = 1.43072296222
03-01-2018:02:50:25,126 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5520, total_loss/examples = 1.430215934
03-01-2018:02:50:31,820 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5530, total_loss/examples = 1.4299353095
03-01-2018:02:50:37,950 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5540, total_loss/examples = 1.42954576326
03-01-2018:02:50:44,752 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5550, total_loss/examples = 1.42911186031
03-01-2018:02:50:50,861 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5560, total_loss/examples = 1.4287460326
03-01-2018:02:50:56,930 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5570, total_loss/examples = 1.42835433932
03-01-2018:02:51:02,740 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5580, total_loss/examples = 1.42808152184
03-01-2018:02:51:08,926 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5590, total_loss/examples = 1.42776323687
03-01-2018:02:51:15,86 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5600, total_loss/examples = 1.42717422476
03-01-2018:02:51:21,141 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5610, total_loss/examples = 1.42683654275
03-01-2018:02:51:27,196 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5620, total_loss/examples = 1.42646881539
03-01-2018:02:51:33,156 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5630, total_loss/examples = 1.42623422
03-01-2018:02:51:38,77 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5640, total_loss/examples = 1.42597853573
03-01-2018:02:51:44,79 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5650, total_loss/examples = 1.42571213848
03-01-2018:02:51:50,283 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5660, total_loss/examples = 1.42532102599
03-01-2018:02:51:56,183 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5670, total_loss/examples = 1.42487058991
03-01-2018:02:52:02,799 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5680, total_loss/examples = 1.42448660284
03-01-2018:02:52:08,732 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5690, total_loss/examples = 1.42402757347
03-01-2018:02:52:14,938 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5700, total_loss/examples = 1.42367851763
03-01-2018:02:52:20,939 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5710, total_loss/examples = 1.42327753577
03-01-2018:02:52:26,941 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5720, total_loss/examples = 1.42289827844
03-01-2018:02:52:31,926 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5730, total_loss/examples = 1.42247661432
03-01-2018:02:52:38,240 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5740, total_loss/examples = 1.42220638509
03-01-2018:02:52:44,335 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5750, total_loss/examples = 1.42168269287
03-01-2018:02:52:50,464 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5760, total_loss/examples = 1.42132524307
03-01-2018:02:52:56,626 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5770, total_loss/examples = 1.42104596595
03-01-2018:02:53:02,826 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5780, total_loss/examples = 1.42066770977
03-01-2018:02:53:08,963 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5790, total_loss/examples = 1.42018168463
03-01-2018:02:53:15,417 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5800, total_loss/examples = 1.41989670807
03-01-2018:02:53:21,534 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5810, total_loss/examples = 1.41966354073
03-01-2018:02:53:27,636 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5820, total_loss/examples = 1.41926572111
03-01-2018:02:53:33,717 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5830, total_loss/examples = 1.41907669376
03-01-2018:02:53:39,875 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5840, total_loss/examples = 1.41864714815
03-01-2018:02:53:45,867 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5850, total_loss/examples = 1.41823455703
03-01-2018:02:53:52,520 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5860, total_loss/examples = 1.41787316643
03-01-2018:02:53:58,416 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5870, total_loss/examples = 1.41760909227
03-01-2018:02:54:03,612 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5880, total_loss/examples = 1.41729878597
03-01-2018:02:54:09,487 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5890, total_loss/examples = 1.41698855156
03-01-2018:02:54:15,393 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5900, total_loss/examples = 1.41658634992
03-01-2018:02:54:21,592 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5910, total_loss/examples = 1.41622344108
03-01-2018:02:54:27,558 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5920, total_loss/examples = 1.41581113003
03-01-2018:02:54:33,754 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5930, total_loss/examples = 1.41536556408
03-01-2018:02:54:39,777 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5940, total_loss/examples = 1.41498414084
03-01-2018:02:54:45,866 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5950, total_loss/examples = 1.41467151876
03-01-2018:02:54:51,986 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5960, total_loss/examples = 1.41429286615
03-01-2018:02:54:57,767 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5970, total_loss/examples = 1.41389055519
03-01-2018:02:55:02,751 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5980, total_loss/examples = 1.41360913165
03-01-2018:02:55:08,665 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 5990, total_loss/examples = 1.41330599004
03-01-2018:02:55:14,564 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6000, total_loss/examples = 1.41290070838
03-01-2018:02:55:20,748 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6010, total_loss/examples = 1.41247581584
03-01-2018:02:55:26,942 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6020, total_loss/examples = 1.41209688528
03-01-2018:02:55:33,275 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6030, total_loss/examples = 1.41160853533
03-01-2018:02:55:39,366 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6040, total_loss/examples = 1.41123040991
03-01-2018:02:55:45,389 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6050, total_loss/examples = 1.41090970014
03-01-2018:02:55:51,261 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6060, total_loss/examples = 1.41059955025
03-01-2018:02:55:57,740 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6070, total_loss/examples = 1.41026013151
03-01-2018:02:56:04,106 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6080, total_loss/examples = 1.41006475579
03-01-2018:02:56:09,917 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6090, total_loss/examples = 1.40973661519
03-01-2018:02:56:16,300 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6100, total_loss/examples = 1.4093641823
03-01-2018:02:56:21,446 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6110, total_loss/examples = 1.40910139633
03-01-2018:02:56:27,401 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6120, total_loss/examples = 1.40878862353
03-01-2018:02:56:33,533 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6130, total_loss/examples = 1.40829044014
03-01-2018:02:56:39,542 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6140, total_loss/examples = 1.40798586102
03-01-2018:02:56:45,454 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6150, total_loss/examples = 1.40778930381
03-01-2018:02:56:51,777 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6160, total_loss/examples = 1.40742076386
03-01-2018:02:56:57,774 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6170, total_loss/examples = 1.40708344867
03-01-2018:02:57:03,664 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6180, total_loss/examples = 1.40674838425
03-01-2018:02:57:09,734 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6190, total_loss/examples = 1.40622178716
03-01-2018:02:57:15,742 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6200, total_loss/examples = 1.40591254562
03-01-2018:02:57:21,715 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6210, total_loss/examples = 1.40562065319
03-01-2018:02:57:27,740 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6220, total_loss/examples = 1.40528002805
03-01-2018:02:57:32,923 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6230, total_loss/examples = 1.40494990097
03-01-2018:02:57:38,942 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6240, total_loss/examples = 1.40462036626
03-01-2018:02:57:45,142 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6250, total_loss/examples = 1.40435064787
03-01-2018:02:57:51,470 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6260, total_loss/examples = 1.40407867878
03-01-2018:02:57:57,355 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6270, total_loss/examples = 1.40375038979
03-01-2018:02:58:03,293 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6280, total_loss/examples = 1.40338816367
03-01-2018:02:58:09,244 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6290, total_loss/examples = 1.40299356365
03-01-2018:02:58:15,571 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6300, total_loss/examples = 1.40255097334
03-01-2018:02:58:21,762 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6310, total_loss/examples = 1.40237358882
03-01-2018:02:58:27,801 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6320, total_loss/examples = 1.4020074833
03-01-2018:02:58:33,547 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6330, total_loss/examples = 1.40167808186
03-01-2018:02:58:40,808 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6340, total_loss/examples = 1.40141803373
03-01-2018:02:58:46,38 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6350, total_loss/examples = 1.40104752408
03-01-2018:02:58:52,68 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6360, total_loss/examples = 1.40074490757
03-01-2018:02:58:58,187 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6370, total_loss/examples = 1.40047641791
03-01-2018:02:59:04,333 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6380, total_loss/examples = 1.40023495032
03-01-2018:02:59:10,459 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6390, total_loss/examples = 1.39983793461
03-01-2018:02:59:16,624 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6400, total_loss/examples = 1.39953615302
03-01-2018:02:59:22,685 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6410, total_loss/examples = 1.39927268423
03-01-2018:02:59:28,521 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6420, total_loss/examples = 1.39895449258
03-01-2018:02:59:34,582 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6430, total_loss/examples = 1.398620756
03-01-2018:02:59:40,609 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6440, total_loss/examples = 1.39830280861
03-01-2018:02:59:45,585 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6450, total_loss/examples = 1.39796286199
03-01-2018:02:59:51,661 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6460, total_loss/examples = 1.39764037561
03-01-2018:02:59:57,525 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6470, total_loss/examples = 1.39722613575
03-01-2018:03:00:03,293 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6480, total_loss/examples = 1.39700780304
03-01-2018:03:00:09,671 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6490, total_loss/examples = 1.39669853928
03-01-2018:03:00:15,911 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6500, total_loss/examples = 1.39642012821
03-01-2018:03:00:21,748 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6510, total_loss/examples = 1.39604394216
03-01-2018:03:00:27,495 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6520, total_loss/examples = 1.39555647308
03-01-2018:03:00:33,376 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6530, total_loss/examples = 1.39527053612
03-01-2018:03:00:40,359 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6540, total_loss/examples = 1.39490129403
03-01-2018:03:00:46,678 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6550, total_loss/examples = 1.39449219841
03-01-2018:03:00:52,896 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6560, total_loss/examples = 1.39415056208
03-01-2018:03:00:58,804 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6570, total_loss/examples = 1.39392434729
03-01-2018:03:01:04,856 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6580, total_loss/examples = 1.39365517756
03-01-2018:03:01:10,859 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6590, total_loss/examples = 1.3932613826
03-01-2018:03:01:16,824 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6600, total_loss/examples = 1.39285540934
03-01-2018:03:01:22,700 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6610, total_loss/examples = 1.39246909011
03-01-2018:03:01:28,832 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6620, total_loss/examples = 1.39213644794
03-01-2018:03:01:34,881 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6630, total_loss/examples = 1.39176905218
03-01-2018:03:01:39,825 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6640, total_loss/examples = 1.39157002028
03-01-2018:03:01:45,766 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6650, total_loss/examples = 1.3913105631
03-01-2018:03:01:51,645 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6660, total_loss/examples = 1.3909293451
03-01-2018:03:01:57,723 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6670, total_loss/examples = 1.39059431776
03-01-2018:03:02:03,762 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6680, total_loss/examples = 1.39037437784
03-01-2018:03:02:09,821 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6690, total_loss/examples = 1.3900101184
03-01-2018:03:02:15,824 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6700, total_loss/examples = 1.38989953192
03-01-2018:03:02:21,950 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6710, total_loss/examples = 1.38964559572
03-01-2018:03:02:27,40 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6720, total_loss/examples = 1.38928236853
03-01-2018:03:02:33,112 INFO     [ASReaderTrainer.py:141] Epoch 2/5, minibatch = 6730, total_loss/examples = 1.38904538843
03-01-2018:03:02:38,934 INFO     [ASReaderTrainer.py:161] Epoch 2/5, total_loss/examples_seen = 1.38882939694
03-01-2018:03:02:40,228 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 0, total_loss/examples = 1.38877022487
03-01-2018:03:02:40,228 INFO     [ASReaderTrainer.py:191] Starting to calculate accuracy
03-01-2018:03:02:40,228 INFO     [ASReaderTrainer.py:185] Starting to predict
03-01-2018:03:05:03,770 INFO     [ASReaderTrainer.py:146] valid_accuracy = 0.647915620291
03-01-2018:03:05:03,771 INFO     [ASReaderTrainer.py:149] previous valid accuracy = 0.619286790558, current valid accuracy = 0.647915620291
03-01-2018:03:05:03,783 INFO     [ASReaderTrainer.py:215] Saving model in file: /home/shantanu/PycharmProjects/attentionSum/generated_data/model_save.txt
03-01-2018:03:05:06,654 INFO     [ASReaderTrainer.py:218] Saving model args in file: /home/shantanu/PycharmProjects/attentionSum/generated_data/model_args_save.txt
03-01-2018:03:05:06,695 INFO     [ASReaderTrainer.py:221] Done saving model and model args
03-01-2018:03:05:12,984 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 10, total_loss/examples = 1.38828018355
03-01-2018:03:05:18,860 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 20, total_loss/examples = 1.38776046613
03-01-2018:03:05:24,968 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 30, total_loss/examples = 1.38725225836
03-01-2018:03:05:31,616 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 40, total_loss/examples = 1.38676327375
03-01-2018:03:05:37,617 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 50, total_loss/examples = 1.38620633926
03-01-2018:03:05:42,668 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 60, total_loss/examples = 1.38578083323
03-01-2018:03:05:48,692 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 70, total_loss/examples = 1.3852412515
03-01-2018:03:05:54,779 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 80, total_loss/examples = 1.38471373242
03-01-2018:03:06:00,717 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 90, total_loss/examples = 1.38424450168
03-01-2018:03:06:06,892 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 100, total_loss/examples = 1.38380241744
03-01-2018:03:06:12,982 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 110, total_loss/examples = 1.38322606862
03-01-2018:03:06:19,13 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 120, total_loss/examples = 1.38276357778
03-01-2018:03:06:25,126 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 130, total_loss/examples = 1.38230137488
03-01-2018:03:06:30,840 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 140, total_loss/examples = 1.38181591649
03-01-2018:03:06:36,780 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 150, total_loss/examples = 1.38128262348
03-01-2018:03:06:41,886 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 160, total_loss/examples = 1.38080180353
03-01-2018:03:06:47,910 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 170, total_loss/examples = 1.38036973285
03-01-2018:03:06:53,942 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 180, total_loss/examples = 1.37997771148
03-01-2018:03:07:00,317 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 190, total_loss/examples = 1.37938811885
03-01-2018:03:07:06,541 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 200, total_loss/examples = 1.3789085713
03-01-2018:03:07:12,733 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 210, total_loss/examples = 1.37851085119
03-01-2018:03:07:18,714 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 220, total_loss/examples = 1.37796967055
03-01-2018:03:07:24,920 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 230, total_loss/examples = 1.37735919586
03-01-2018:03:07:31,458 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 240, total_loss/examples = 1.37679983041
03-01-2018:03:07:37,584 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 250, total_loss/examples = 1.37636723994
03-01-2018:03:07:43,671 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 260, total_loss/examples = 1.37578060005
03-01-2018:03:07:49,746 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 270, total_loss/examples = 1.37529120934
03-01-2018:03:07:56,62 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 280, total_loss/examples = 1.37482523779
03-01-2018:03:08:01,979 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 290, total_loss/examples = 1.37433646144
03-01-2018:03:08:07,19 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 300, total_loss/examples = 1.37379040973
03-01-2018:03:08:12,763 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 310, total_loss/examples = 1.37327924758
03-01-2018:03:08:18,884 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 320, total_loss/examples = 1.37277991992
03-01-2018:03:08:25,32 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 330, total_loss/examples = 1.37231511634
03-01-2018:03:08:31,77 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 340, total_loss/examples = 1.37184641981
03-01-2018:03:08:36,975 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 350, total_loss/examples = 1.37132916365
03-01-2018:03:08:42,937 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 360, total_loss/examples = 1.37088963641
03-01-2018:03:08:49,72 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 370, total_loss/examples = 1.37047217053
03-01-2018:03:08:54,369 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 380, total_loss/examples = 1.36996675915
03-01-2018:03:09:00,602 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 390, total_loss/examples = 1.36951848486
03-01-2018:03:09:06,741 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 400, total_loss/examples = 1.3690917233
03-01-2018:03:09:12,808 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 410, total_loss/examples = 1.36858978594
03-01-2018:03:09:18,988 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 420, total_loss/examples = 1.36810773567
03-01-2018:03:09:25,27 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 430, total_loss/examples = 1.36758679264
03-01-2018:03:09:31,304 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 440, total_loss/examples = 1.36727024223
03-01-2018:03:09:37,426 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 450, total_loss/examples = 1.36682869003
03-01-2018:03:09:43,427 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 460, total_loss/examples = 1.36628211768
03-01-2018:03:09:49,620 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 470, total_loss/examples = 1.36581631841
03-01-2018:03:09:55,958 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 480, total_loss/examples = 1.36531207509
03-01-2018:03:10:01,60 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 490, total_loss/examples = 1.36482004596
03-01-2018:03:10:07,25 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 500, total_loss/examples = 1.36432026616
03-01-2018:03:10:13,135 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 510, total_loss/examples = 1.36384206286
03-01-2018:03:10:19,396 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 520, total_loss/examples = 1.36338423316
03-01-2018:03:10:25,786 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 530, total_loss/examples = 1.3629551832
03-01-2018:03:10:31,826 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 540, total_loss/examples = 1.36245593461
03-01-2018:03:10:37,782 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 550, total_loss/examples = 1.36192930577
03-01-2018:03:10:44,133 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 560, total_loss/examples = 1.36143791822
03-01-2018:03:10:50,173 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 570, total_loss/examples = 1.36103538356
03-01-2018:03:10:56,376 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 580, total_loss/examples = 1.36056091068
03-01-2018:03:11:01,857 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 590, total_loss/examples = 1.36008719058
03-01-2018:03:11:07,929 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 600, total_loss/examples = 1.35964419664
03-01-2018:03:11:14,125 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 610, total_loss/examples = 1.35923800823
03-01-2018:03:11:20,274 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 620, total_loss/examples = 1.35879918412
03-01-2018:03:11:26,331 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 630, total_loss/examples = 1.35830795227
03-01-2018:03:11:32,288 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 640, total_loss/examples = 1.35773684574
03-01-2018:03:11:39,26 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 650, total_loss/examples = 1.35721559893
03-01-2018:03:11:45,55 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 660, total_loss/examples = 1.35672905017
03-01-2018:03:11:51,125 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 670, total_loss/examples = 1.35624051857
03-01-2018:03:11:56,223 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 680, total_loss/examples = 1.35580905679
03-01-2018:03:12:02,384 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 690, total_loss/examples = 1.355313515
03-01-2018:03:12:08,527 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 700, total_loss/examples = 1.35500735688
03-01-2018:03:12:14,812 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 710, total_loss/examples = 1.35461770548
03-01-2018:03:12:20,842 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 720, total_loss/examples = 1.3540744948
03-01-2018:03:12:26,872 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 730, total_loss/examples = 1.35359081971
03-01-2018:03:12:33,67 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 740, total_loss/examples = 1.35317025666
03-01-2018:03:12:39,122 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 750, total_loss/examples = 1.35273044142
03-01-2018:03:12:45,320 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 760, total_loss/examples = 1.35229528589
03-01-2018:03:12:51,513 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 770, total_loss/examples = 1.35178719048
03-01-2018:03:12:57,699 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 780, total_loss/examples = 1.35121339545
03-01-2018:03:13:03,220 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 790, total_loss/examples = 1.35074167374
03-01-2018:03:13:09,203 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 800, total_loss/examples = 1.35038815766
03-01-2018:03:13:15,509 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 810, total_loss/examples = 1.34988734119
03-01-2018:03:13:21,613 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 820, total_loss/examples = 1.34936444674
03-01-2018:03:13:27,474 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 830, total_loss/examples = 1.34892798975
03-01-2018:03:13:34,493 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 840, total_loss/examples = 1.34841354721
03-01-2018:03:13:40,514 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 850, total_loss/examples = 1.34807629307
03-01-2018:03:13:46,638 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 860, total_loss/examples = 1.34757376357
03-01-2018:03:13:52,526 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 870, total_loss/examples = 1.34717839298
03-01-2018:03:13:57,844 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 880, total_loss/examples = 1.34679025824
03-01-2018:03:14:05,808 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 890, total_loss/examples = 1.34644718198
03-01-2018:03:14:13,930 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 900, total_loss/examples = 1.34595355649
03-01-2018:03:14:19,945 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 910, total_loss/examples = 1.34550910248
03-01-2018:03:14:26,140 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 920, total_loss/examples = 1.34499240542
03-01-2018:03:14:32,36 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 930, total_loss/examples = 1.34455886277
03-01-2018:03:14:38,93 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 940, total_loss/examples = 1.34413620697
03-01-2018:03:14:44,148 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 950, total_loss/examples = 1.3437052496
03-01-2018:03:14:50,248 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 960, total_loss/examples = 1.34327606276
03-01-2018:03:14:56,237 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 970, total_loss/examples = 1.34290455539
03-01-2018:03:15:02,272 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 980, total_loss/examples = 1.34243338143
03-01-2018:03:15:08,197 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 990, total_loss/examples = 1.34192695685
03-01-2018:03:15:14,177 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1000, total_loss/examples = 1.34166067029
03-01-2018:03:15:20,347 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1010, total_loss/examples = 1.34124343231
03-01-2018:03:15:26,379 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1020, total_loss/examples = 1.34078316952
03-01-2018:03:15:31,502 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1030, total_loss/examples = 1.34030303272
03-01-2018:03:15:37,491 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1040, total_loss/examples = 1.33999972493
03-01-2018:03:15:43,837 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1050, total_loss/examples = 1.33957781659
03-01-2018:03:15:50,147 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1060, total_loss/examples = 1.33914830889
03-01-2018:03:15:56,545 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1070, total_loss/examples = 1.33874593844
03-01-2018:03:16:02,562 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1080, total_loss/examples = 1.33824872045
03-01-2018:03:16:08,660 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1090, total_loss/examples = 1.33784797018
03-01-2018:03:16:14,637 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1100, total_loss/examples = 1.33739198597
03-01-2018:03:16:20,738 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1110, total_loss/examples = 1.33703588339
03-01-2018:03:16:27,215 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1120, total_loss/examples = 1.33667306031
03-01-2018:03:16:33,188 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1130, total_loss/examples = 1.33620107987
03-01-2018:03:16:39,42 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1140, total_loss/examples = 1.33569737581
03-01-2018:03:16:44,950 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1150, total_loss/examples = 1.3352746549
03-01-2018:03:16:49,958 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1160, total_loss/examples = 1.33484399621
03-01-2018:03:16:55,835 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1170, total_loss/examples = 1.33448045178
03-01-2018:03:17:02,5 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1180, total_loss/examples = 1.33410650858
03-01-2018:03:17:07,936 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1190, total_loss/examples = 1.33370281214
03-01-2018:03:17:14,94 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1200, total_loss/examples = 1.33331714217
03-01-2018:03:17:20,125 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1210, total_loss/examples = 1.33291924876
03-01-2018:03:17:26,228 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1220, total_loss/examples = 1.33249736956
03-01-2018:03:17:32,357 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1230, total_loss/examples = 1.33216773005
03-01-2018:03:17:38,418 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1240, total_loss/examples = 1.33171770352
03-01-2018:03:17:44,323 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1250, total_loss/examples = 1.3314236192
03-01-2018:03:17:49,417 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1260, total_loss/examples = 1.33097741702
03-01-2018:03:17:55,440 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1270, total_loss/examples = 1.33062067688
03-01-2018:03:18:01,416 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1280, total_loss/examples = 1.33025997865
03-01-2018:03:18:07,461 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1290, total_loss/examples = 1.32988934934
03-01-2018:03:18:13,516 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1300, total_loss/examples = 1.3294441033
03-01-2018:03:18:19,368 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1310, total_loss/examples = 1.32898603992
03-01-2018:03:18:25,399 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1320, total_loss/examples = 1.32852419528
03-01-2018:03:18:31,356 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1330, total_loss/examples = 1.32802756894
03-01-2018:03:18:37,265 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1340, total_loss/examples = 1.32765619207
03-01-2018:03:18:43,231 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1350, total_loss/examples = 1.32724944659
03-01-2018:03:18:48,266 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1360, total_loss/examples = 1.32671366945
03-01-2018:03:18:54,367 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1370, total_loss/examples = 1.32630813738
03-01-2018:03:19:00,260 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1380, total_loss/examples = 1.3258686566
03-01-2018:03:19:06,972 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1390, total_loss/examples = 1.32537066619
03-01-2018:03:19:13,26 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1400, total_loss/examples = 1.32489233645
03-01-2018:03:19:18,989 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1410, total_loss/examples = 1.32447868677
03-01-2018:03:19:25,65 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1420, total_loss/examples = 1.32401474592
03-01-2018:03:19:31,65 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1430, total_loss/examples = 1.32363160859
03-01-2018:03:19:37,238 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1440, total_loss/examples = 1.32317768667
03-01-2018:03:19:43,628 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1450, total_loss/examples = 1.32279582563
03-01-2018:03:19:49,962 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1460, total_loss/examples = 1.32231884299
03-01-2018:03:19:55,880 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1470, total_loss/examples = 1.32190226491
03-01-2018:03:20:02,79 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1480, total_loss/examples = 1.32150488772
03-01-2018:03:20:07,934 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1490, total_loss/examples = 1.32100788847
03-01-2018:03:20:14,703 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1500, total_loss/examples = 1.32058723648
03-01-2018:03:20:20,698 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1510, total_loss/examples = 1.32018089169
03-01-2018:03:20:25,682 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1520, total_loss/examples = 1.31972319453
03-01-2018:03:20:31,668 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1530, total_loss/examples = 1.31942575329
03-01-2018:03:20:37,733 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1540, total_loss/examples = 1.31901667743
03-01-2018:03:20:44,52 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1550, total_loss/examples = 1.31872038267
03-01-2018:03:20:50,430 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1560, total_loss/examples = 1.31831279709
03-01-2018:03:20:56,292 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1570, total_loss/examples = 1.31786890125
03-01-2018:03:21:02,655 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1580, total_loss/examples = 1.31745134331
03-01-2018:03:21:08,853 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1590, total_loss/examples = 1.31702399279
03-01-2018:03:21:14,620 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1600, total_loss/examples = 1.31660435923
03-01-2018:03:21:20,525 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1610, total_loss/examples = 1.3161611553
03-01-2018:03:21:26,501 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1620, total_loss/examples = 1.3157136255
03-01-2018:03:21:32,495 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1630, total_loss/examples = 1.31526626827
03-01-2018:03:21:37,487 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1640, total_loss/examples = 1.31491219397
03-01-2018:03:21:43,587 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1650, total_loss/examples = 1.31452745784
03-01-2018:03:21:49,922 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1660, total_loss/examples = 1.31411508052
03-01-2018:03:21:56,110 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1670, total_loss/examples = 1.31381988555
03-01-2018:03:22:02,273 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1680, total_loss/examples = 1.31337561606
03-01-2018:03:22:08,600 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1690, total_loss/examples = 1.31299903538
03-01-2018:03:22:14,526 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1700, total_loss/examples = 1.31258718846
03-01-2018:03:22:20,532 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1710, total_loss/examples = 1.31217108433
03-01-2018:03:22:25,614 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1720, total_loss/examples = 1.31172739141
03-01-2018:03:22:32,475 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1730, total_loss/examples = 1.31144431869
03-01-2018:03:22:37,515 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1740, total_loss/examples = 1.31101079147
03-01-2018:03:22:43,585 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1750, total_loss/examples = 1.31061466924
03-01-2018:03:22:49,702 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1760, total_loss/examples = 1.31034492912
03-01-2018:03:22:55,686 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1770, total_loss/examples = 1.30986281949
03-01-2018:03:23:02,300 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1780, total_loss/examples = 1.30948674403
03-01-2018:03:23:08,61 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1790, total_loss/examples = 1.30906829864
03-01-2018:03:23:14,94 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1800, total_loss/examples = 1.30869932723
03-01-2018:03:23:20,77 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1810, total_loss/examples = 1.30842226322
03-01-2018:03:23:26,205 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1820, total_loss/examples = 1.30799879457
03-01-2018:03:23:32,90 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1830, total_loss/examples = 1.30756764218
03-01-2018:03:23:38,211 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1840, total_loss/examples = 1.30716087222
03-01-2018:03:23:44,71 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1850, total_loss/examples = 1.30671032527
03-01-2018:03:23:50,18 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1860, total_loss/examples = 1.30630121479
03-01-2018:03:23:55,950 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1870, total_loss/examples = 1.30589924056
03-01-2018:03:24:01,133 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1880, total_loss/examples = 1.30551832811
03-01-2018:03:24:08,93 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1890, total_loss/examples = 1.30513299282
03-01-2018:03:24:13,224 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1900, total_loss/examples = 1.30469893251
03-01-2018:03:24:19,147 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1910, total_loss/examples = 1.3043395251
03-01-2018:03:24:25,87 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1920, total_loss/examples = 1.30395812132
03-01-2018:03:24:31,350 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1930, total_loss/examples = 1.30370222616
03-01-2018:03:24:37,427 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1940, total_loss/examples = 1.30326289169
03-01-2018:03:24:43,587 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1950, total_loss/examples = 1.30286726038
03-01-2018:03:24:49,728 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1960, total_loss/examples = 1.30259231274
03-01-2018:03:24:55,641 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1970, total_loss/examples = 1.30216956141
03-01-2018:03:25:01,369 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1980, total_loss/examples = 1.30178031945
03-01-2018:03:25:07,315 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 1990, total_loss/examples = 1.3013597106
03-01-2018:03:25:13,236 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2000, total_loss/examples = 1.30092787208
03-01-2018:03:25:18,517 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2010, total_loss/examples = 1.30045976093
03-01-2018:03:25:24,464 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2020, total_loss/examples = 1.30005496736
03-01-2018:03:25:30,511 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2030, total_loss/examples = 1.29975933924
03-01-2018:03:25:36,327 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2040, total_loss/examples = 1.2995009075
03-01-2018:03:25:42,221 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2050, total_loss/examples = 1.29914759345
03-01-2018:03:25:48,373 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2060, total_loss/examples = 1.29871282391
03-01-2018:03:25:54,396 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2070, total_loss/examples = 1.29830291015
03-01-2018:03:26:00,376 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2080, total_loss/examples = 1.29795679244
03-01-2018:03:26:06,392 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2090, total_loss/examples = 1.29757222319
03-01-2018:03:26:12,452 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2100, total_loss/examples = 1.29715910428
03-01-2018:03:26:18,52 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2110, total_loss/examples = 1.29679831366
03-01-2018:03:26:23,988 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2120, total_loss/examples = 1.2963702188
03-01-2018:03:26:30,109 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2130, total_loss/examples = 1.29601351066
03-01-2018:03:26:36,13 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2140, total_loss/examples = 1.29561811633
03-01-2018:03:26:42,354 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2150, total_loss/examples = 1.29522536264
03-01-2018:03:26:48,788 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2160, total_loss/examples = 1.29484409316
03-01-2018:03:26:54,719 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2170, total_loss/examples = 1.29446353762
03-01-2018:03:27:00,820 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2180, total_loss/examples = 1.29407611298
03-01-2018:03:27:06,769 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2190, total_loss/examples = 1.2937316212
03-01-2018:03:27:11,946 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2200, total_loss/examples = 1.29342447066
03-01-2018:03:27:17,710 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2210, total_loss/examples = 1.29308384468
03-01-2018:03:27:23,762 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2220, total_loss/examples = 1.29273078264
03-01-2018:03:27:30,65 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2230, total_loss/examples = 1.29233020556
03-01-2018:03:27:36,524 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2240, total_loss/examples = 1.29196814665
03-01-2018:03:27:42,563 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2250, total_loss/examples = 1.29162244607
03-01-2018:03:27:48,467 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2260, total_loss/examples = 1.29130537542
03-01-2018:03:27:54,293 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2270, total_loss/examples = 1.29097570771
03-01-2018:03:28:00,364 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2280, total_loss/examples = 1.29064365118
03-01-2018:03:28:06,953 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2290, total_loss/examples = 1.29035026303
03-01-2018:03:28:13,432 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2300, total_loss/examples = 1.29005018544
03-01-2018:03:28:19,473 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2310, total_loss/examples = 1.28977486572
03-01-2018:03:28:25,778 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2320, total_loss/examples = 1.28950372502
03-01-2018:03:28:32,65 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2330, total_loss/examples = 1.28905222556
03-01-2018:03:28:37,208 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2340, total_loss/examples = 1.28862386052
03-01-2018:03:28:44,97 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2350, total_loss/examples = 1.28826810091
03-01-2018:03:28:50,313 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2360, total_loss/examples = 1.28789462744
03-01-2018:03:28:56,329 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2370, total_loss/examples = 1.2875276426
03-01-2018:03:29:02,626 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2380, total_loss/examples = 1.28713140588
03-01-2018:03:29:08,723 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2390, total_loss/examples = 1.28678302408
03-01-2018:03:29:14,888 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2400, total_loss/examples = 1.28643864465
03-01-2018:03:29:21,89 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2410, total_loss/examples = 1.28594999343
03-01-2018:03:29:27,300 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2420, total_loss/examples = 1.28557821756
03-01-2018:03:29:33,360 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2430, total_loss/examples = 1.28519100193
03-01-2018:03:29:39,409 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2440, total_loss/examples = 1.28478328477
03-01-2018:03:29:44,365 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2450, total_loss/examples = 1.2844565401
03-01-2018:03:29:50,504 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2460, total_loss/examples = 1.28404840468
03-01-2018:03:29:56,461 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2470, total_loss/examples = 1.28362288626
03-01-2018:03:30:03,720 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2480, total_loss/examples = 1.28330634566
03-01-2018:03:30:09,866 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2490, total_loss/examples = 1.28297830542
03-01-2018:03:30:16,703 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2500, total_loss/examples = 1.28267827069
03-01-2018:03:30:23,338 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2510, total_loss/examples = 1.28230189083
03-01-2018:03:30:29,511 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2520, total_loss/examples = 1.28198425535
03-01-2018:03:30:35,660 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2530, total_loss/examples = 1.28167258838
03-01-2018:03:30:42,782 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2540, total_loss/examples = 1.28129249281
03-01-2018:03:30:48,967 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2550, total_loss/examples = 1.28096468644
03-01-2018:03:30:54,970 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2560, total_loss/examples = 1.28054930725
03-01-2018:03:31:01,389 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2570, total_loss/examples = 1.2802736894
03-01-2018:03:31:07,482 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2580, total_loss/examples = 1.27996928832
03-01-2018:03:31:13,476 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2590, total_loss/examples = 1.27963393236
03-01-2018:03:31:19,695 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2600, total_loss/examples = 1.27928667654
03-01-2018:03:31:25,753 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2610, total_loss/examples = 1.27896424246
03-01-2018:03:31:30,989 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2620, total_loss/examples = 1.27862937825
03-01-2018:03:31:37,831 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2630, total_loss/examples = 1.27822009071
03-01-2018:03:31:44,78 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2640, total_loss/examples = 1.2778860155
03-01-2018:03:31:50,191 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2650, total_loss/examples = 1.27750489591
03-01-2018:03:31:56,550 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2660, total_loss/examples = 1.27728466389
03-01-2018:03:32:02,720 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2670, total_loss/examples = 1.2769407286
03-01-2018:03:32:08,884 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2680, total_loss/examples = 1.2765245984
03-01-2018:03:32:15,278 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2690, total_loss/examples = 1.27624014293
03-01-2018:03:32:22,159 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2700, total_loss/examples = 1.27593722263
03-01-2018:03:32:28,396 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2710, total_loss/examples = 1.2756244741
03-01-2018:03:32:34,453 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2720, total_loss/examples = 1.27527283504
03-01-2018:03:32:40,549 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2730, total_loss/examples = 1.27494919167
03-01-2018:03:32:47,470 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2740, total_loss/examples = 1.27468263593
03-01-2018:03:32:53,510 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2750, total_loss/examples = 1.27435725588
03-01-2018:03:32:59,646 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2760, total_loss/examples = 1.27404535535
03-01-2018:03:33:05,798 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2770, total_loss/examples = 1.273772701
03-01-2018:03:33:11,822 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2780, total_loss/examples = 1.27343116286
03-01-2018:03:33:18,24 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2790, total_loss/examples = 1.27306312958
03-01-2018:03:33:24,159 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2800, total_loss/examples = 1.27269538461
03-01-2018:03:33:32,85 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2810, total_loss/examples = 1.2723696226
03-01-2018:03:33:38,165 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2820, total_loss/examples = 1.27191207635
03-01-2018:03:33:44,117 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2830, total_loss/examples = 1.27159177255
03-01-2018:03:33:50,292 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2840, total_loss/examples = 1.27117789537
03-01-2018:03:33:56,445 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2850, total_loss/examples = 1.27080056585
03-01-2018:03:34:02,522 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2860, total_loss/examples = 1.27041357818
03-01-2018:03:34:08,557 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2870, total_loss/examples = 1.27004433329
03-01-2018:03:34:14,466 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2880, total_loss/examples = 1.26970095844
03-01-2018:03:34:21,330 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2890, total_loss/examples = 1.26936779233
03-01-2018:03:34:27,180 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2900, total_loss/examples = 1.26891621133
03-01-2018:03:34:33,338 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2910, total_loss/examples = 1.26860915854
03-01-2018:03:34:38,338 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2920, total_loss/examples = 1.26830279036
03-01-2018:03:34:44,991 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2930, total_loss/examples = 1.26792988224
03-01-2018:03:34:51,44 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2940, total_loss/examples = 1.267562854
03-01-2018:03:34:57,77 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2950, total_loss/examples = 1.26716763885
03-01-2018:03:35:03,28 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2960, total_loss/examples = 1.26678410816
03-01-2018:03:35:08,898 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2970, total_loss/examples = 1.2664571234
03-01-2018:03:35:14,882 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2980, total_loss/examples = 1.26606713013
03-01-2018:03:35:21,9 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 2990, total_loss/examples = 1.2658314149
03-01-2018:03:35:27,42 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3000, total_loss/examples = 1.2655108637
03-01-2018:03:35:32,854 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3010, total_loss/examples = 1.2651682887
03-01-2018:03:35:38,21 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3020, total_loss/examples = 1.26477193853
03-01-2018:03:35:45,223 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3030, total_loss/examples = 1.26449365108
03-01-2018:03:35:51,464 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3040, total_loss/examples = 1.26426209541
03-01-2018:03:35:56,474 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3050, total_loss/examples = 1.26382425622
03-01-2018:03:36:02,350 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3060, total_loss/examples = 1.26351809626
03-01-2018:03:36:08,653 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3070, total_loss/examples = 1.26322192494
03-01-2018:03:36:14,739 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3080, total_loss/examples = 1.26296569746
03-01-2018:03:36:20,540 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3090, total_loss/examples = 1.26264669692
03-01-2018:03:36:26,539 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3100, total_loss/examples = 1.26237374537
03-01-2018:03:36:32,562 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3110, total_loss/examples = 1.26210084207
03-01-2018:03:36:38,569 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3120, total_loss/examples = 1.26175755623
03-01-2018:03:36:44,765 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3130, total_loss/examples = 1.26142511039
03-01-2018:03:36:49,947 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3140, total_loss/examples = 1.26103149439
03-01-2018:03:36:55,980 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3150, total_loss/examples = 1.26071150828
03-01-2018:03:37:02,100 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3160, total_loss/examples = 1.26030182961
03-01-2018:03:37:08,233 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3170, total_loss/examples = 1.26004149179
03-01-2018:03:37:14,312 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3180, total_loss/examples = 1.25978724255
03-01-2018:03:37:20,419 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3190, total_loss/examples = 1.25948189573
03-01-2018:03:37:26,196 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3200, total_loss/examples = 1.2590906967
03-01-2018:03:37:32,476 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3210, total_loss/examples = 1.25876322133
03-01-2018:03:37:38,403 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3220, total_loss/examples = 1.25845965157
03-01-2018:03:37:44,353 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3230, total_loss/examples = 1.25816536286
03-01-2018:03:37:50,396 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3240, total_loss/examples = 1.25790516017
03-01-2018:03:37:55,814 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3250, total_loss/examples = 1.2576215872
03-01-2018:03:38:01,804 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3260, total_loss/examples = 1.25728362313
03-01-2018:03:38:07,790 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3270, total_loss/examples = 1.25698683855
03-01-2018:03:38:13,741 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3280, total_loss/examples = 1.25665415354
03-01-2018:03:38:20,208 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3290, total_loss/examples = 1.25633216199
03-01-2018:03:38:26,169 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3300, total_loss/examples = 1.25609456905
03-01-2018:03:38:32,53 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3310, total_loss/examples = 1.25576325972
03-01-2018:03:38:37,867 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3320, total_loss/examples = 1.25543208638
03-01-2018:03:38:43,943 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3330, total_loss/examples = 1.25512010853
03-01-2018:03:38:49,310 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3340, total_loss/examples = 1.25498236876
03-01-2018:03:38:55,603 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3350, total_loss/examples = 1.25472529416
03-01-2018:03:39:01,678 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3360, total_loss/examples = 1.25441250246
03-01-2018:03:39:07,534 INFO     [ASReaderTrainer.py:141] Epoch 3/5, minibatch = 3370, total_loss/examples = 1.2540652277
03-01-2018:03:39:09,579 INFO     [ASReaderTrainer.py:191] Starting to calculate accuracy
03-01-2018:03:39:09,579 INFO     [ASReaderTrainer.py:185] Starting to predict
03-01-2018:03:41:33,47 INFO     [ASReaderTrainer.py:146] valid_accuracy = 0.637870416876
03-01-2018:03:41:33,48 INFO     [ASReaderTrainer.py:149] previous valid accuracy = 0.647915620291, current valid accuracy = 0.637870416876
03-01-2018:03:41:33,48 INFO     [ASReaderTrainer.py:153] previous accuracy > current accuracy. Stopping to train
03-01-2018:03:41:33,49 INFO     [ASReaderModel.py:136] Calculating accuracy with 2491 data points
03-01-2018:03:41:33,49 INFO     [ASReaderTrainer.py:191] Starting to calculate accuracy
03-01-2018:03:41:33,49 INFO     [ASReaderTrainer.py:185] Starting to predict
03-01-2018:03:44:39,412 INFO     [ASReaderModel.py:138] accuracy = 0.592934564432
03-01-2018:03:44:39,412 INFO     [main.py:137] test_accuracy = 0.592934564432
03-01-2018:03:44:39,425 INFO     [ASReaderTrainer.py:215] Saving model in file: /home/shantanu/PycharmProjects/attentionSum/generated_data/model_save.txt
03-01-2018:03:44:42,299 INFO     [ASReaderTrainer.py:218] Saving model args in file: /home/shantanu/PycharmProjects/attentionSum/generated_data/model_args_save.txt
03-01-2018:03:44:42,342 INFO     [ASReaderTrainer.py:221] Done saving model and model args
