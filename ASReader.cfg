[Logging]
log_level = info
log_file = /home/shantanu/PycharmProjects/attentionSum/as_reader.log
logger = ASReader

[Data]
cbt_data_dir = /home/shantanu/PycharmProjects/attentionSum/CBTest/data
cbtne_train_file = cbtest_NE_train.txt
cbtne_valid_file = cbtest_NE_valid_2000ex.txt
cbtne_test_file = cbtest_NE_test_2500ex.txt
generated_data_dir = /home/shantanu/PycharmProjects/attentionSum/generated_data
vocab_file = as_reader_vocab.txt
w2i_file = as_reader_w2i.txt
train_save_file = cbtest_NE_train_save.txt
valid_save_file = cbtest_NE_valid_save.txt
test_save_file = cbtest_NE_test_save.txt
model_save_file = model_save.txt
model_args_save_file = model_args_save.txt
keep_top_vocab_percentage = 90.0
max_data_points = None
should_load_saved_data = false

[ExecutionMode]
mode = training

[ModelParameters]
emb_dim = 128
gru_layers = 1
gru_input_dim = 128
gru_hidden_dim = 128
num_of_unk = 10
should_load_saved_model = false

[TrainingParameters]
adam_alpha = 0.001
minibatch_size = 16
n_epochs = 2
gradient_clipping_thresh = 10.0
lookup_init_scale = 1.0
n_times_predict_in_epoch = 2
should_save_model_while_training = true

